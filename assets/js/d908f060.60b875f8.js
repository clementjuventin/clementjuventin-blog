"use strict";(self.webpackChunkpersonal_website=self.webpackChunkpersonal_website||[]).push([[890],{161:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"building-an-arbitrager","metadata":{"permalink":"/clementjuventin-blog/building-an-arbitrager","source":"@site/blog/2025-06-03-building-an-arbitrager.md","title":"Building an Arbitrage Bot","description":"I\'ve been fascinated by MEV (Maximal Extractable Value) for a while now, and over the past few weeks, I\u2019ve started building my own arbitrage bot. It\u2019s been a challenging yet insightful journey, and I\u2019d like to share my experience so far.","date":"2025-06-03T00:00:00.000Z","tags":[{"inline":false,"label":"Web3","permalink":"/clementjuventin-blog/tags/web3","description":"Web3 related projects"},{"inline":false,"label":"Story","permalink":"/clementjuventin-blog/tags/story","description":"Story about my life"},{"inline":false,"label":"Technical","permalink":"/clementjuventin-blog/tags/technical","description":"Technical articles"}],"readingTime":11.89,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"building-an-arbitrager","title":"Building an Arbitrage Bot","authors":"clementjuventin","tags":["web3","story","technical"]},"unlisted":false,"nextItem":{"title":"From A Nooby Degen to a Bloodthirsty Sniper","permalink":"/clementjuventin-blog/degen-to-sniper"}},"content":"I\'ve been fascinated by **MEV** (Maximal Extractable Value) for a while now, and over the past few weeks, I\u2019ve started building **my own arbitrage bot**. It\u2019s been a challenging yet insightful journey, and I\u2019d like to share my experience so far.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is MEV?\\n\\nPeople trade funds, people watch them trading funds, people found ways to make profits based on funds being traded.\\nTo be honest, this is not today\'s topic, and it\'s a very vast field. To learn more, [this article may help you](https://www.coingecko.com/learn/what-is-mev-maximal-extractable-value-crypto).\\n\\n## Starting with the Basics\\n\\nTo begin this adventure, I decided to explore the implementation of a **CowSwap solver**.\\n\\nFor those unfamiliar, [CowSwap](https://cow.fi/cow-protocol) is an off-chain order book where solvers **compete to submit the most efficient on-chain settlement**. Since all solvers use the same underlying liquidity sources, the competition lies less in speed or on-chain monitoring tricks, and more in **algorithmic quality** (from what I understand).\\n\\nI figured this would be a great way to get hands-on experience with arbitrage using real-world tools, while also learning from the CowSwap ecosystem itself.\\n\\nUnfortunately, that turned out to be a bit optimistic. The competitiveness of an arbitrage system is deeply tied to its performance \u2014 which means that sharing strategies often **undermines profitability**. So, I quickly realized that there wouldn\u2019t be much help available when it came to solving the core problems.\\n\\nThat said, I followed CowSwap\u2019s documentation closely and was able to get the full local stack up and running, including syncing with the live order book.\\n\\nI chose to implement my solver in **C++** \u2014 for once, it wasn\u2019t overkill. I was surprised to find I was already familiar with many of the liquidity models: **Uniswap V2/V3**, **Balancer\u2019s stable and weighted pools**, and a few others. The remaining pool types seemed negligible.\\n\\n### Then the Real Challenge Hit\\n\\nThe complexity came when I had to actually **write the algorithm** \u2014 and I had no clear direction. Neither the documentation nor any online resource offered much guidance.\\n\\nAn ideal solver algorithm would likely involve:\\n\\n- **Graph theory**, to identify optimal trade paths\\n- **Game theory**, to balance solver profitability with order competitiveness\\n- **Mathematical modeling**, to simulate off-chain liquidity movements\\n- Handling **limit orders**, **partial orders**, and other edge cases\\n\\nIn short, this problem is likely **NP-complete**, requiring advanced algorithms and a deep mathematical foundation. I had just thrown myself headfirst into the deep end of arbitrage complexity.\\n\\n## Can You Still Get Results Without a PhD Team?\\n\\nThat\u2019s the question I began asking myself. Is it possible to **simplify the problem** and/or **reduce the level of competition**? It was clear that I had no chance to compete with the big players, even after a few months of development.\\n\\nOne idea came to mind: target a **newer blockchain** \u2014 one that\u2019s less mature in terms of ecosystem and developer tooling. That would mean less competition, and maybe a better learning ground.\\n\\n### Enter HyperEVM\\n\\nMy search led me to **HyperEVM**, a relatively new EVM-compatible blockchain developed by [**Hyperliquid**](https://hyperliquid.gitbook.io/hyperliquid-docs). It felt like stepping into the Wild West.\\n\\nWhy **HyperEVM** was promising:\\n\\n- **Low gas fees**\\n- **Modest but exploitable trading volume**\\n- **EVM compatibility** (making integration much easier)\\n- **Fewer active arbitrage bots**, due to its early-stage ecosystem\\n\\nSo I began roughing out a system that could serve as an arbitrage bot on this new chain. I\u2019ll describe the different directions I explored to improve the bot\u2019s profitability. I experimented as much as I could and tried to learn by doing, testing every idea that seemed viable.\\n\\n## The Adventure Begins\\n\\nChoosing an EVM-compatible chain seemed like a great idea \u2014 in theory, it would allow me to port this system to any other EVM-compatible chain later on. In practice, though, things turned out to be more complicated due to protocols specificities.\\n\\n### The First Proof of Concept\\n\\nTo build my proof of concept (PoC), I needed a few key components:\\n\\n- Fast access to on-chain data\\n- Off-chain liquidity simulation for Uniswap-like pools\\n- The ability to send on-chain transactions with protection against slippage\\n\\nI started with a **QuickNode RPC**, but quickly hit the 200 requests/sec limit. That capped me at monitoring roughly 30 pools during testing.\\n\\nI integrated [**Hyperswap**](https://hyperswap.exchange/) and [**Kittenswap**](https://kittenswap.finance/), the two most active DEXs on **HyperEVM**.\\n\\nMy first approach, somewhat naive, was to look for arbitrage loops that start and end with the same token. That way, there\'s no need to rely on price approximations (e.g., A \u2192 B \u2192 A or A \u2192 B \u2192 C \u2192 A). I deployed a smart contract capable of executing multiple swaps and reverting if the final balance was lower than the initial one. This ensure that the bot will never spend more than it earns.\\n\\n```solidity\\nfunction execute(\\n    Trade[] calldata trades,\\n    uint256 amountIn,\\n    address startToken\\n) external payable returns (uint256) {\\n    uint256 balanceBefore = IERC20(startToken).balanceOf(address(this));\\n\\n    // ... perform trades ...\\n\\n    uint256 balanceAfter = IERC20(startToken).balanceOf(address(this));\\n    // This line will revert all the previous operations and stop the execution\\n    require(balanceAfter > balanceBefore, \\"Arbitrage failed\\");\\n\\n    return balanceAfter - balanceBefore;\\n}\\n```\\n\\nIn TypeScript, I implemented functions to compute expected token outputs for Uniswap V2 and approximated Uniswap V3 swaps \u2014 V3 is more complex due to its tick system while V2 is pretty straightforward.\\n\\nTo detect arbitrage opportunities, I wrote a simple brute force algorithm that computes the best arbitrage path for a given input amount.\\n\\nI decided to hardcode the input amount to 1 HYPE, and to ignore the gas cost of the transaction because it was extremely low at the time.\\n\\n\\n### The First Result\\n\\nWith everything wired up, I ran the bot, stepped away for 20 minutes\u2026 and came back to see my **first profitable transaction \u2014 about $0.30** at the time.\\n\\n![First Arbitrage Transaction](/img/first_arbitrage_tx.jpg)\\n> My first arbitrage transaction on HyperEVM. Source [Purrsec](https://purrsec.com).\\n\\nIt was a small gain, but a huge morale boost. At that point, I believed I was close to a viable system. With a dedicated node and estimated infrastructure costs around $100/month, profitability seemed achievable.\\n\\n**Spoiler**: I still had a long road ahead. Competition would ramp up quickly in the days to come. But in hindsight, that initial optimism was crucial \u2014 if I had only focused on the months of work and potential financial losses ahead, I might have stopped right there.\\n\\n## Moving Toward Production\\n\\nSo far, here\'s a diagram of my architecture \u2014 each module is explained below.\\n\\n![Architecture](/img/arbitrager_archi.png)\\n> Colors are just here to make things prettier \ud83d\udc4c\u200b.\\n\\n### HyperEVM Custom Node\\n\\nMy first step toward production was setting up **a local node**. This gave me full on-chain access with low latency and no rate limits. I didn\'t follow the advice from the documentation, which recommended using a node in Japan to reduce latency issues, even though it was cheaper in the US. I eventually switched and ended up losing several days syncing the node with a machine in Tokyo due to latency issues.\\n\\n### Cold data Indexer\\n\\nI needed a way to cache persistent data (e.g., token names, decimals, pool tokens). So I wrote an **indexer that stores data in Redis**, refreshed periodically.\\n\\n### Solvers\\n\\nEach solver receives a request from the driver and solves a graph to find arbitrage paths. Initially, this logic was in **JavaScript** for rapid prototyping. Later, I migrated to **C++**, which was 80x faster on average \u2014 crucial when tracking many pools. It is the same brute force algorithm I used in the PoC.\\n\\n```cpp\\ninline void find_best_arbitrage_cycle(\\n    const std::vector<std::vector<const Api::LiquidityPoolTradePath *>> &all_cycles,\\n    const Api::uint256 amount_in, \\n    Api::uint256 &best_amount_out,\\n    std::vector<const Api::LiquidityPoolTradePath *> &best_cycle\\n    ) {\\n  for (const auto &cycle : all_cycles) {\\n    try {\\n      const Api::uint256 amount_out = compute_cycle_amount_out(cycle, amount_in);\\n\\n      if (amount_out > best_amount_out) {\\n        best_amount_out = amount_out;\\n        best_cycle = cycle;\\n      }\\n\\n    } catch (Math::InvalidLiquidityStateError &e) {\\n      continue;\\n    } catch (const std::exception &e) {\\n      std::cout << \\"Error: \\" << e.what() << std::endl;\\n    }\\n  }\\n}\\n```\\n\\nNote that I was heavily inspired by CowSwap. It was important for me to design the system with multiple solvers from the start, as this would allow for horizontal scaling of the solving process, as well as safe comparison of different algorithms \u2014 since each solution is evaluated and put in competition with the others.\\n\\n### Monitoring \\nTo monitor performance, I built a metrics indexer that feeds **Prometheus**, which powers **Grafana** dashboards.\\n\\nTo be honest, I under-invested here. I\'m still missing key insights and can\u2019t precisely track profit/loss over time. The idea was to compare strategies and versions of the arbitrage engine, but I\u2019ve mostly been busy building essential features.\\n\\nYou can see in the following screenshot that this allows me to visualize the increase in the contract\'s balances (due to arbitrage activity) and the amount of HYPE available to cover transaction fees for the wallets responsible for calling the arbitrageur.\\n\\n![Grafana dashboard](/img/arbitrager_dashboard.png)\\n> Grafana dashboard (with a bug regarding BTC decimals).\\n\\n### Driver\\n\\nI built a central coordination module called the **Driver** (also inspired by CowSwap\u2019s architecture). It\u2019s responsible for:\\n\\n- High-frequency data collection\\n- Structuring the data into a standard request format\\n- Sending requests to all available solvers\\n- Collecting and evaluating responses within a strict timeout\\n- Comparing the results to determine the best solver and submit the transaction\\n\\nSimulating gas cost and submitting transactions reliably was another challenge. I eventually introduced a dynamic fee deduction, setting aside a portion of profit to cover validator fees and ensure transaction inclusion. I am still not convinced with this approach, but I have no better idea for now.\\n\\nThanks to the strategy of using multiple solvers, I was able to narrow down the list of cryptocurrencies used to initiate arbitrage cycles. This increases the number of opportunities, as it allows for shorter arbitrage loops.\\n\\nFor example, a cycle like A \u2192 B \u2192 C \u2192 B \u2192 A can be shortened to B \u2192 C \u2192 B, saving two transactions if the contract already holds B.\\n\\nResponses are evaluated and filtered \u2014 in the PoC, I hardcoded the input token amount to 1 HYPE, which was suboptimal. Profits vary significantly depending on input size.\\n\\nTo omptimize the input size estimation, I tried different strategies:\\n\\n#### Offchain computation\\n\\nOff-chain calculation is fast and reliable \u2014 if you have accurate models and complete data. For Uniswap V2, the math is simple. For V3, it\u2019s [**evil**](https://uniswapv3book.com/index.html).\\n\\nIn the PoC, I used a rough approximation to avoid tick-level math. To compute real outputs with Uniswap V3\'s SDK, you need **all tick data**, which is extremely resource-intensive \u2014 even with a private node.\\n\\nMy idea was to fetch a subset of ticks because arbitrage transactions are often low-volume. I managed to code the math but the delays were still significant when it came to fetch the data. I finally gave up this approach for the next strategy.\\n\\n#### Onchain computation\\n\\nInstead of fully computing everything off-chain, I opted for a hybrid approach:\\n\\n- Approximate swaps using a constant-tick assumption (ie. Uniswap V3 math approximation)\\n- Collect all promising trade paths\\n- Simulate each candidate on-chain via a custom smart contract (a read-only call)\\n- Use binary search (dichotomy) to find the input amount that maximizes profit\\n\\nThis approach is based on the fact that the data is available, but communication between the driver and the node results in significant latency. Even when both programs run on the same machine, each request still has to go through the full network stack. Since tick retrieval can\'t be properly parallelized, fetch times become a bottleneck.\\n\\nInstead, the node is tasked with simulating a smart contract execution. This way, it uses the same math as the actual contracts (eliminating the risk of errors) and has direct, optimized access to its own data.\\n\\nThis gave me much better results while avoiding heavy off-chain computation. Here is a snippet of the binary search function, it returns the optimal `k` value (ie. the best amount) to maximize the profit:\\n\\n```solidity\\nfunction binarySearchOptimalK(\\n    Trade[] calldata trades,\\n    uint256 step,\\n    uint256 balance\\n) internal returns (uint256) {\\n    uint256 low_k = 1;\\n    uint256 high_k = balance / step;\\n    while (low_k < high_k) {\\n        uint256 mid_k = low_k + (high_k - low_k) / 2;\\n        uint256 profitMid = computeTradesProfit(\\n            trades,\\n            mid_k * step\\n        );\\n        uint256 profitNext = computeTradesProfit(\\n            trades,\\n            (mid_k + 1) * step\\n        );\\n        if (profitMid >= profitNext) {\\n            high_k = mid_k;\\n        } else {\\n            low_k = mid_k + 1;\\n        }\\n    }\\n    return low_k;\\n}\\n```\\n\\nAnd this is **costless** because I am **simulating** this call (even though it\'s a write function).\\n\\n### Smart Contract Module\\n\\nMy first smart contract was simple, and it worked well initially.\\nThe main purpose is to prevent the bot to concretely loose funds (e.g. Spend 1 A and obtain 0.98 A).\\n\\nBut as my whole bot system matured, it became clear that the contract was **critical to profitability**.\\n\\nProfit calculation is straightforward: `profit = total_output - total_input - gas_cost`.\\n\\nBut when a transaction fails, you still pay the gas \u2014 often due to a competing bot front-running your trade.\\nThese failed transactions became increasingly costly due to the gas price increasing of a ~500x factor (HyperEVM became more popular).\\n\\n![HyperEVM daily transactions](/img/hyperevm_daily_tx.png)\\n> HyperEVM daily transactions. Source [Purrsec](https://purrsec.com/trending/metrics).\\n\\nTo mitigate this:\\n\\n- I optimized gas usage with basic Solidity gas optimizations\\n- Introduced an early-exit condition before the final balance comparison (`balanceAfter < balanceBefore`)\\n- Tweaked execution to minimize unnecessary operations\\n\\nThis lowered the average cost per trade significantly, but not enough.\\n\\n## Current Results & Next Steps\\n\\nAs of now, the bot is not yet profitable. On average, I recover only ~80% of the input.\\n\\nTo bridge the gap to profitability, I have two main ideas:\\n\\n- Reduce fees further\\n- Use heuristics to detect and avoid unprofitable trades\\n\\nThe second option seems most promising \u2014 I haven\u2019t explored it yet, and it requires a larger dataset to analyze trade outcomes at scale. I\'ll need to analyze all trades performed and hopefully find patterns that reduce the number of failed transactions.\\n\\nAlso, I face a new challenge: **latency**.\\n\\nBlock time on HyperEVM has dropped to under 1 second (2 seconds originally). Because my experiments run from a machine in US, network latency to the HyperEvm sequencers (hosted in Tokyo) is hurting my competitiveness.\\n\\nI\u2019ll need a new setup \u2014 ideally a server located closer to the sequencer region \u2014 to reduce latency and stay in sync.\\n\\n## Final Thoughts\\n\\nThere\u2019s still a long road ahead, but I believe it\u2019s worth it. I\u2019ve learned a ton about arbitrage, on-chain mechanics, and smart contract optimization \u2014 and I\u2019m only getting started."},{"id":"degen-to-sniper","metadata":{"permalink":"/clementjuventin-blog/degen-to-sniper","source":"@site/blog/2025-03-25-degen-to-sniper.md","title":"From A Nooby Degen to a Bloodthirsty Sniper","description":"In this article I\'d like to recount my first steps as a degen and what prompted me to turn to the world of sniping.","date":"2025-03-25T00:00:00.000Z","tags":[{"inline":false,"label":"Web3","permalink":"/clementjuventin-blog/tags/web3","description":"Web3 related projects"},{"inline":false,"label":"Story","permalink":"/clementjuventin-blog/tags/story","description":"Story about my life"},{"inline":false,"label":"Technical","permalink":"/clementjuventin-blog/tags/technical","description":"Technical articles"}],"readingTime":8.57,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"degen-to-sniper","title":"From A Nooby Degen to a Bloodthirsty Sniper","authors":"clementjuventin","tags":["web3","story","technical"]},"unlisted":false,"prevItem":{"title":"Building an Arbitrage Bot","permalink":"/clementjuventin-blog/building-an-arbitrager"},"nextItem":{"title":"Opal - Technical Overview","permalink":"/clementjuventin-blog/opal-technical-overview"}},"content":"In this article I\'d like to recount my *first steps as a degen* and what prompted me to turn to the **world of sniping**.\\n\\n\x3c!-- truncate --\x3e\\n\\nUp until recently, I had always kept an eye on the web3 and blockchain space from a distance. I followed the news, attended events and conferences, and got to know many of the key players who set the **foundations and rules of the ecosystem**.\\n\\nBut there was always this one group that felt almost mythical to me \u2014 the **degens**. A close friend of mine was part of what I used to label as the degen community. He belonged to private groups focused on **alpha leaks, technical collaboration, and all sorts of niche financial services**. For a long time, it all seemed a bit unproductive to me from a personal growth standpoint, so I kept my distance.\\n\\nOver time, however, I began to realize that beyond the financial aspect, there was a real sense of connection \u2014 a network that could be nurtured and leveraged as an additional driver for success. To be honnest, I also had more time than ever for this kind of activity.\\n\\nSo I decided to dive in and start my journey as a degen. It turned out to be much more challenging than I had anticipated. I quickly discovered that this world demands an **almost full-time commitment**. It\u2019s a game of **hunting, anticipating, and executing transactions** based on public data. Starting out on **Base** (Coinbase\'s blockchain), I quickly lost my first few hundred dollars. But I knew the potential was there. After a few weeks, I managed to break even and eventually started seeing small profits.\\n\\nAs the weeks passed, I identified some of my key strengths and weaknesses.\\n\\n**Strengths**:\\n\\n- I have a technical background. I can understand the intricate details of projects and deeply master the tools needed to operate effectively in this space.\\n- I have valuable contacts in the industry who\u2019ve generously shared their knowledge and saved me months of trial and error.\\n\\n**Weaknesses**:\\n\\n- I work solo. Despite those contacts, I can\u2019t rely on them indefinitely. In the degen world, data is money. I can\u2019t expect them to constantly feed me information. Also, being a solo operator means I\u2019m limited by time, whereas groups can distribute the workload and share information more efficiently.\\n\\nThat\u2019s when I decided to team up with an old friend who was already well-established in the ecosystem. He was also a dev and had access to valuable information. I, on the other hand, had time and solid technical discipline to support him on various projects.\\n\\nTogether, we began developing a range of analytics and monitoring tools. Without going into too much detail, we built:\\n- AlphaGate monitoring\\n- Twitter activity monitors\\n- Factory contract monitors on Base\\n- Onchain OSINT for Twitter/Warpcaster addresses\\n- Onchain Data analytics tools\\n\\nAll these projects were just the beginning \u2014 a warm-up for something much bigger: diving into **crypto sniping**.\\n\\n## What Is Sniping?\\n\\nBy sniping, I mean setting up a system to **quickly buy a cryptocurrency just before it catches a wave of attention**. Imagine a highly anticipated token is about to launch. The market hasn\u2019t priced it in yet, and for a brief moment, it\u2019s undervalued. A sniper aims to e**xploit that gap** \u2014 buying at launch before the market corrects.\\n\\nThis practice is often frowned upon \u2014 and understandably so. When you buy early with the sole intent to flip for a quick profit, you inevitably hurt retail investors who get in later and might immediately find themselves in the red. Indirectly, this also harms the project itself.\\n\\nBut in the end, the only rule that truly matters is the market. Sniping is a fierce game \u2014 full of traps, failures, and intense competition. Being consistently profitable is just as difficult as in any other degen strategy. I\u2019ve had bots miss the entry entirely while friends manually secured massive gains.\\n\\nIn the coming post, I\u2019ll break down some of the sniping techniques I\u2019ve personally developed \u2014 along with a few wild anecdotes from the field.\\n\\n![Pepe Sniper](/img/sad_pepe_sniper.png)\\n\\n## Sniping Techniques\\n\\nThere are many different sniping techniques, and after watching countless token launches, I can confidently say that **no two are ever the same**. Sometimes projects announce the contract on Twitter, other times on Telegram, through a custom website interface, or via a launchpad. You always need to do some investigation to anticipate which sniping strategy has the highest chance of working.\\n\\nThe two sniping techniques that have brought me the most consistent profits are the following.\\n\\n### Official Claim Sniper\\n\\nWhat I call the *official claim sniper* is a bot that waits for a verified announcement from a trustworthy source before buying the token. Scammers often deploy fake versions of tokens, so without direct confirmation from the project team, jumping in right after deployment is **extremely risky**. Some snipers take that gamble anyway, assuming the potential profits outweigh the possible losses. Personally, I prefer the safer route, even if it means lower profit multipliers \u2014 **risk management comes first**.\\n\\nThis type of bot is relatively simple. It only requires a **notification module** and a **transaction module**. A typical architecture I\u2019ve used involves multiple purchase microservices (one per chain), all exposed through a standard API, along with a separate notification microservice configured per opportunity to monitor data and trigger the buy. Hosting both services on the same machine is important to maximize speed.\\n\\nFor example, you could monitor tweets from a memecoin on Avalanche, extract any EVM address with a regex, and buy on whatever DEX has liquidity for that token.\\n\\nIn theory, it sounds simple. But in practice, **there are many potential points of failure** that can ruin the operation.\\n\\nFirst, the competition is **brutal**. Everyone\u2019s sniper bots are tuned to fire in the same seconds. The first few moments after a hyped token launch usually show a **huge price spike**. That\'s because the earliest snipers are buying aggressively, inflating the price before the market stabilizes. If your bot isn\u2019t fast enough, you\u2019ll end up buying at the top and instantly taking a loss as early buyers dump their tokens. You need to strike a balance: be early enough to profit, but not so aggressive that you pay too much in gas or slippage.\\n\\n![KAIKU/WETH pair on Base](/img/dexscreener.com_KAIKU_WETH_2025-06-06_00-37-34.png)\\n> KAIKU/WETH pair on Base (at launch), one candle = 2s (ie block time). Source: [Dexscreener](https://dexscreener.com/base/0x4c9498a3f36709ee57b1b7c4b440d8481a1b9f79).\\n\\nSecond, the projects themselves have started fighting back. Many view sniping as toxic behavior and implement countermeasures:\\n\\n- Obfuscating the contract address (e.g., inserting extra characters or posting it as an image)\\n- Sharing a link to the DEX liquidity pool instead of the contract directly\\n- Using redirect links to custom sites where the contract is revealed\\n- Deploying a fake token first, then warning the community about it\\n- Enabling extreme transaction taxes for the first few seconds to penalize snipers\\n\\nAll these traps carry real risks \u2014 from simply **missing the opportunity** to **losing your entire investment**. With solid preparation and strategies involving AI or semi-public information feeds, you can sometimes bypass these tricks faster than a human could.\\n\\nA great example of how dangerous this can get is detailed in [this excellent post](https://tactical.deepwaterstudios.xyz/p/anti-sniper-tech-custom-dex), which explains how anti-bot tactics drained massive amounts from automated snipers. We nearly fell victim ourselves during that period. While I was casually skiing down a slope, waiting for a midday launch, my partner was glued to Twitter. He spotted a Discord message revealing the project team\u2019s plan was to bait snipers with a fake contract. On reflex, he shut down our script. One minute later, a fake contract tweet dropped \u2014 and just like that, another sniper lost $75,000.\\n\\nWe were shaken. Even though we weren\u2019t risking that much, it was enough to seriously hurt. Ironically, the competitor immediately re-entered the market with the same amount (what confidence!). In the end, both their bot and ours failed to enter the market, likely due to the a technical hiccup. But the experience was a sobering reminder: even with good tooling and timing, **the risk is never zero**.\\n\\n### Block Zero Sniper\\n\\nThis technique relies on knowledge of the launchpad that will host the token launch, as well as additional details like the deployer address or token ticker. The idea is to deploy a smart contract that sends a flood of buy transactions every block, attempting to snipe within the same block the liquidity pool is created.\\n\\nThis is possible because many launchpads (like Virtuals) use a factory contract that updates a counter or state variable when a new pair is created. By monitoring that state at high frequency, the sniper can instantly detect a new pool and trigger a buy. Ideally, you filter targets using the ticker or deployer to avoid blindly buying into every new token.\\n\\nThis reminds me of an interesting anecdote from Binance Smart Chain. CZ, Binance\u2019s founder, had announced he would reveal the name of his dog at 16:00 UTC \u2014 a seemingly harmless statement that degens took very seriously. Why? Because they knew people would rush to create tokens named after the dog, sparking a frenzy of speculation. One sniper had the brilliant idea to create a smart contract with $10,000, programmed to invest $1,000 into each of the first 10 tokens deployed after 16:00 UTC.\\n\\nYou\u2019ve got to admire that level of confidence \u2014 blindly throwing tens of thousands into what could be worthless tokens. But in this case, it paid off. Tree of the ten tokens was related to CZ\'s post \u2014 BROCOLLI (the dog) \u2014 made that sniper a millionaire in under 3 seconds. WTF bro what if CZ wanted a beer at 16:00 UTC and posted at 16:01?\\n\\n## Conclusion\\n\\nAs you\'ve seen, sniping techniques vary wildly, and **the risks are as real as the rewards**. In practice, we only make profitable trades in about 1 out of every 5 attempts. **Most of the time, nothing happens**.\\n\\nOne of the biggest lessons I\u2019ve learned is that technical sophistication alone doesn\u2019t guarantee success. Often, creativity, originality, and unconventional thinking bring more value than raw code. Always keep an open mind and approach every opportunity like a new challenge \u2014 because in this game, that\'s exactly what it is."},{"id":"opal-technical-overview","metadata":{"permalink":"/clementjuventin-blog/opal-technical-overview","source":"@site/blog/2025-01-23-opal-technical-overview.md","title":"Opal - Technical Overview","description":"In this article, I\u2019ll dive into the technical underpinnings of the Opal protocol, a project I\u2019ve contributed to extensively. If you haven\u2019t yet read the previous article about Opal, I recommend starting there to better understand the context of what follows.","date":"2025-01-23T00:00:00.000Z","tags":[{"inline":false,"label":"Opal","permalink":"/clementjuventin-blog/tags/opal","description":"Opal articles"},{"inline":false,"label":"Technical","permalink":"/clementjuventin-blog/tags/technical","description":"Technical articles"},{"inline":false,"label":"Web3","permalink":"/clementjuventin-blog/tags/web3","description":"Web3 related projects"}],"readingTime":9.83,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"opal-technical-overview","title":"Opal - Technical Overview","authors":"clementjuventin","tags":["opal","technical","web3"]},"unlisted":false,"prevItem":{"title":"From A Nooby Degen to a Bloodthirsty Sniper","permalink":"/clementjuventin-blog/degen-to-sniper"},"nextItem":{"title":"My first Open Source Contribution in C++ - EVMOne","permalink":"/clementjuventin-blog/contribution-to-open-source-projects-evmone"}},"content":"In this article, I\u2019ll dive into the **technical underpinnings of the Opal protocol**, a project I\u2019ve contributed to extensively. If you haven\u2019t yet read the previous article about Opal, I recommend starting [there](./2024-09-15-opal.md) to better understand the context of what follows.\\n\\n\x3c!-- truncate --\x3e\\n\\nHere, we\u2019ll focus on two of the protocol\u2019s most important smart contracts: the **Omnipool** and the **Reward Manager**.\\n\\nThe Omnipool is the core of the protocol\u2014it\u2019s a single-asset liquidity pool that can dynamically rebalance and reallocate funds across different strategies. It lies at the heart of Opal\u2019s yield-generation mechanism and represents the foundation of the platform\'s financial logic.\\n\\nThe second contract, the Reward Manager, is one I developed almost entirely independently. Its role is to handle the distribution of rewards to users participating in the Omnipool. I find the mathematical model behind it particularly elegant and effective. While similar systems are likely common and well-studied in DeFi, in this article, we\u2019ll take a closer look at Opal\u2019s unique implementation.\\n\\n## Protocol Overview\\n\\n![Protocol Overview](/img/opal_design.png)\\n\\nThis diagram is a simplified version of the Opal protocol, highlighting the **flow of liquidity and rewards** managed by the protocol.\\n\\nThe typical operation of an omnipool is as follows:\\n- For an omnipool that accepts an underlying asset A, the governance module proposes a set of pools along with a distribution weight for the liquidity.\\n- Liquidity deposited into the omnipool is allocated according to these weights.\\n- If the current underlying pool weights deviate from those voted by the governance module, an incentive in GEM (the governance token) is distributed to reward rebalancing. The pool weights represents the relative share of the liquidity allocated to each underlying pool.\\n- Over time, the omnipool accrues rewards, which are tracked by the Reward Manager.\\n- When a user withdraws their liquidity or chooses to claim their rewards, the Reward Manager distributes the appropriate amount.\\n\\n## The Omnipool\\n\\nLet\u2019s start by examining the main method of the omnipool: **the deposit method**.\\n\\n> As a general note for this article: in all code snippets, I systematically remove uninteresting sections such as parameter validation to improve readability.\\n\\n```solidity\\nfunction depositFor(uint256 _amountIn, address _depositFor, uint256 _minLpReceived) public {\\n    // Get the price of the underlying asset in USD\\n    uint256 underlyingPrice = oracle.getUSDPrice(address(underlyingToken));\\n\\n    // Estimate the value in USD of the liquidity in the omnipool before the deposit\\n    uint256 beforeTotalUnderlying = _getTotalAndPerPoolUnderlying(underlyingPrice);\\n    \\n    // Transfer underlying token to this contract\\n    underlyingToken.safeTransferFrom(msg.sender, address(this), _amountIn);\\n\\n    // Estimate the current exchange rate of the omnipool (ie. how much LP tokens are minted for each underlying token deposited)\\n    uint256 exchangeRate = _exchangeRate(beforeTotalUnderlying);\\n\\n    // Deposit into Aura Finance (and Balancer V2)\\n    _depositToAura(beforeTotalUnderlying, _amountIn);\\n\\n    // Estimate the new value of the liquidity in the omnipool after the deposit\\n    uint256 afterTotalUnderlying = _getTotalAndPerPoolUnderlying(underlyingPrice);\\n\\n    // Calculate the increase in the value of the liquidity in the omnipool\\n    uint256 underlyingBalanceIncrease = afterTotalUnderlying - beforeTotalUnderlying;\\n\\n    // Calculate the amount of LP tokens that can be minted\\n    uint256 mintableUnderlyingAmount = _min(_amountIn, underlyingBalanceIncrease);\\n    uint256 lpReceived = mintableUnderlyingAmount.divDown(exchangeRate);\\n\\n    // Slippage protection\\n    if (lpReceived < _minLpReceived) {\\n        revert TooMuchSlippage();\\n    }\\n    lpToken.mint(_depositFor, lpReceived, _depositFor);\\n\\n    // Handle rebalancing rewards\\n    _handleRebalancingRewards(...);\\n\\n    // Emit an event to notify the outside world\\n    emit Deposit(_depositFor, _amountIn, lpReceived);\\n}\\n```\\n\\nYou\'ll immediately notice a key feature of contracts dealing with omnipools: **everything is denominated in USD**, thanks to oracles.\\nIndeed, to maintain a fair balance across multiple underlying pools, it\'s absolutely essential to **accurately price the current value of all positions held by the omnipool**.\\n\\nOpal relies heavily on oracle usage. Every token in every underlying pool must have a reliable price oracle.\\n\\nThis introduces a natural limitation: **it\'s not possible to farm liquidity pools with low market cap tokens**, as they often lack trustworthy oracles. Generally, these tokens offer higher APRs because they carry more risk. It\'s unfortunate to miss out on such opportunities, but this restriction is necessary to ensure the protocol\u2019s overall security.\\n\\nTo conclude on the oracle topic, note that Opal is restricted to use [Chainlink](https://data.chain.link/streams) and [Redstone](https://app.redstone.finance/app/tokens/?tokenTypes=crypto) feeds. These lists define the full range of tokens eligible for integration into new underlying pools.\\n\\nTo estimate the value of a position held by the omnipool, we rely on:\\n- the USD price of each token forming the underlying Balancer pool, and\\n- the math provided in the Balancer documentation, which governs pool behavior.\\n\\n[This document](https://github.com/balancer/docs/blob/main/docs/concepts/advanced/valuing-bpt/valuing-bpt.md#on-chain-price-evaluation) provides a detailed explanation of the math used to estimate the value of a Balancer Pool Token (BPT).\\n\\nAs for the rest, I won\u2019t go deeper into the omnipool methods in this article.\\nHowever, if you\'re interested in exploring the full implementation, the smart contract code is publicly available on [Etherscan](https://etherscan.io/address/0x50953a9842b1fe42db077f1b850ec365a25f9d8e#code).\\n\\n## The Reward Manager\\n\\nLet\u2019s now discuss the **Reward Manager** \u2014 a key module responsible for distributing rewards to users.\\n\\nMore specifically, the rewards accumulated by the omnipool are distributed by Balancer and Aura Finance in the form of their respective governance tokens: `BAL` and `AURA`.\\n\\nIn some cases, external protocols may incentivize users to provide liquidity by offering extra rewards. These are one-off or special tokens distributed in addition to `BAL` and `AURA`. For simplicity, the following examples focus only on handling `BAL` tokens, but the logic can be duplicated to support any other reward token.\\n\\nIt\u2019s also worth noting a crucial difference from Uniswap:\\nIn Uniswap, a liquidity pool A/B typically distributes claimable rewards in both token A and B.\\nIn Balancer V2, however, profits are directly reinvested into the BPT (Balancer Pool Tokens), increasing their intrinsic value.\\n\\nThis means there\u2019s no need to redistribute LP token rewards \u2014 only the reward tokens emitted by the underlying protocols integrated into Opal.\\n\\n### Modeling and Distributing Rewards\\n\\nBefore diving into the code, let\u2019s break down the **mathematical model** behind reward accumulation and distribution.\\n\\nTo accurately track the rewards accrued by the omnipool over time, we use a mathematical integral \u2014 denoted as R (for Rewards). An integral is monotonic and non-decreasing, which perfectly matches our use case:\\n\\n> The omnipool can only accumulate rewards over time, never lose them.\\n\\nTo be more precise, R expresses the values of one unit of LP token in the omnipool over time.\\n\\nAs a simplified example, imagine a linear curve representing R over time \u2014 a case where the omnipool receives a constant flow of rewards and is updated at each block:\\n\\n![Rewards Curve](/img/reward_curve.png)\\n> `R(t) = constant \xd7 t` (very simple case). Source Desmos.\\n\\nWhen a user joins the omnipool, we need to store the value `R(t)` at that moment, representing the accumulated rewards before their participation.\\n\\nWhen the user decides to claim their rewards, we simply:\\n- Retrieve the current reward state `R(t+1)`\\n- Subtract the value at their join time `R(t)`\\n- Compute the reward delta: `reward = R(t+1) - R(t)`\\n\\nThis delta is always \u2265 0 and represents the rewards accrue by one unit of the LP token in the omnipool at `t+1` minus the value at `t`.\\n\\n![Rewards Curve User](/img/reward_curve_user.png)\\n> On this diagram, `t=5` and `t+1=10`. The rewards accrued by one unit of the LP token in the omnipool between `t` and `t+1` is `R(t+1) - R(t) = 10 - 5 = 5`. Source Desmos.\\n\\nNow, the remaining step is to distribute the rewards among liquidity providers proportionally to their share in the pool. This share is known, as it directly corresponds to their current balance. The final formula is:\\n\\n`reward = balance * (R(t+1) - R(t))` (we don\'t see `totalSupply` in the formula because it\'s contained in `R`).\\n\\n### Implementation\\n\\nNow that we have been through the mathematical model, let\'s take a look at the `updateUserState` function, which is called whenever a user deposits or claims rewards. It updates both the global omnipool state and the individual user\u2019s reward data:\\n\\n```solidity\\nfunction updateUserState(address _account) public {\\n    // Get the user\'s LP balance\\n    uint256 deposited = omnipool.balanceOf(_account);\\n\\n    // Update the pool state, claim rewards, and transfer them to the Reward Manager\\n    _updateOmnipoolState();\\n\\n    // Update the user\'s pending rewards\\n    _updateRewards(_account, deposited);\\n}\\n```\\n\\nThe logic is straightforward, let\'s see what\'s inside of `_updateOmnipoolState`:\\n\\n```solidity\\nfunction _updateOmnipoolState() internal {\\n    // Claim rewards accrued from underlying protocols\\n    uint256 earnedBAL = _claimOmnipoolRewards();\\n\\n    // Get the total amount of LP tokens deposited into the omnipool\\n    uint256 totalDeposited = omnipool.totalSupply();\\n\\n    // Update the global reward state\\n    BALMeta.earnedIntegral += (earnedBAL * SCALED_ONE) / totalDeposited;\\n    BALMeta.lastEarned += earnedBAL;\\n\\n    // Update the last balance of BAL in the omnipool\\n    lastBALBalance = IERC20(BAL).balanceOf(address(omnipool));\\n\\n    // Emit an event to notify the claimed rewards\\n    emit RewardUpdated(earnedBAL);\\n}\\n```\\n\\nHere we perform:\\n- Reward collection via _claimOmnipoolRewards()\\n- Update of the reward integral (R) for the BAL token. As we can see, `BALMeta.earnedIntegral` represents the earned BAL rewards devided by the total supply of LP tokens (ie. the reward accrued by one unit of LP token).\\n- Update of the last balance of BAL in the omnipool\\n\\nYou probably noticed that `BALMeta` is a struct used to store reward accounting data for a specific token \u2014 in this case, BAL. See the full struct below:\\n\\n```solidity\\nstruct RewardMeta {\\n    uint256 earnedIntegral; // a scaled running total of reward per unit of LP token (R)\\n    uint256 lastEarned; // the last total amount of reward received\\n    mapping(address => uint256) accountIntegral; // the value of R at the time of the user\'s last update\\n    mapping(address => uint256) accountShare; // amount of reward to distribute to the user\\n}\\n```\\n\\nIt stores both global and per-user reward accounting data.\\n\\nFinally, let\'s take a look at the `_updateRewards` function, which updates the user\'s reward share and integral:\\n\\n```solidity\\nfunction _updateRewards(address account, uint256 balance) internal {\\n    // Get the difference between the global and per-user reward integral (ie. R(t+1) - R(t))\\n    uint256 BALIntegralDelta = BALMeta.earnedIntegral - BALMeta.accountIntegral[account];\\n\\n    // Calculate the amount of reward to distribute to the user\\n    uint256 balShare = (balance * BALIntegralDelta) / SCALED_ONE;\\n\\n    // Update the user\'s reward share\\n    BALMeta.accountShare[account] += balShare;\\n\\n    // Update the user\'s reward integral\\n    BALMeta.accountIntegral[account] = BALMeta.earnedIntegral;\\n\\n    // Emit an event to notify the claimed rewards\\n    emit RewardUpdated(account, BALIntegralDelta);\\n}\\n```\\n\\nThis logic ensures:\\n- Users only earn rewards for periods where they were actively staked\\n- The reward share is fairly proportional to their balance and holding duration\\n\\nThe final piece of the puzzle is the `claimEarnings` function. It updates the state of the omnipool and the user, then transfers the accrued rewards to the user\'s address.\\n\\n```solidity\\nfunction claimEarnings() external {\\n    // Update the user\'s reward state\\n    updateUserState(msg.sender);\\n\\n    // Get the share amounts and reset them to 0\\n    uint256 balAmount = BALMeta.accountShare[msg.sender];\\n    BALMeta.accountShare[msg.sender] = 0;\\n\\n    // Transfer the rewards to the user\\n    BALToken.safeTransferFrom(address(this), msg.sender, balAmount);\\n    lastBALBalance = IERC20(BAL).balanceOf(address(this));\\n\\n    emit RewardClaimed(msg.sender, balAmount);\\n}\\n```\\n\\nYou can find the full implementation of the Reward Manager [here](https://etherscan.io/address/0xc1094a067b862e57678c4bd5e9d27ed2bd4be937#code).\\n\\n### Toward a More Concrete Example\\n\\nNow that you\'ve seen the mathematical model and its implementation, I just wanted to show you a more realistic example to keep in mind.\\n\\nIn the graph below, we represent the evolution of `R(t)`. As you can see, the progression takes the form of a step function. This is because the accrued rewards are only updated from the Reward Manager\u2019s perspective when the corresponding functions are called.\\n\\nAdditionally, the evolution of `R(t)` is not linear. It depends on both the liquidity of the omnipool and the APRs of the underlying pools that compose it.\\n\\n![Rewards Curve Example](/img/reward_curve_complex.png)\\n> `R(t)` more realistic evolution. Source Desmos.\\n\\n## Conclusion\\n\\nLet\'s wrap up this article on the inner workings of the Omnipool and the Reward Manager. I hope it helped you gain a better understanding of the Opal protocol and introduced you to some interesting mechanisms you might consider for your own future work. There are other noteworthy components we haven\u2019t covered here, such as the governance module and the rebalancing system\u2014areas I\u2019ve been less involved with. But perhaps that\u2019ll be a good reason to write a second article. See you soon!"},{"id":"contribution-to-open-source-projects-evmone","metadata":{"permalink":"/clementjuventin-blog/contribution-to-open-source-projects-evmone","source":"@site/blog/2024-10-05-contribution-to-open-source-projects-evmone.md","title":"My first Open Source Contribution in C++ - EVMOne","description":"Recently, I decided to learn the C++ programming language. After a few weeks of study, I felt the need to get hands-on experience with professional code by contributing to open source projects. In this article, I\u2019ll walk you through my very first contribution to an open source project: EVMOne.","date":"2024-10-05T00:00:00.000Z","tags":[{"inline":false,"label":"Web3","permalink":"/clementjuventin-blog/tags/web3","description":"Web3 related projects"},{"inline":false,"label":"Technical","permalink":"/clementjuventin-blog/tags/technical","description":"Technical articles"},{"inline":false,"label":"Story","permalink":"/clementjuventin-blog/tags/story","description":"Story about my life"}],"readingTime":5.73,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"contribution-to-open-source-projects-evmone","title":"My first Open Source Contribution in C++ - EVMOne","authors":"clementjuventin","tags":["web3","technical","story"]},"unlisted":false,"prevItem":{"title":"Opal - Technical Overview","permalink":"/clementjuventin-blog/opal-technical-overview"},"nextItem":{"title":"Opal - Yield DApp built on top of Balancer V2","permalink":"/clementjuventin-blog/opal"}},"content":"Recently, I decided to learn the **C++ programming language**. After a few weeks of study, I felt the need to get hands-on experience with professional code by contributing to open source projects. In this article, I\u2019ll walk you through my very first contribution to an open source project: **EVMOne**.\\n\\n\x3c!-- truncate --\x3e\\n\\n[EVMOne](https://github.com/ethereum/evmone) is an Ethereum Virtual Machine (EVM) implementation written in C++. It can be used as an **execution module** within other Ethereum clients such as [Geth](https://github.com/ethereum/go-ethereum). One of the key advantages of EVMOne is its use of C++, a language well-known for its excellent runtime performance. Theoretically, assuming equivalent implementation, EVMOne should offer the **best execution performance among EVMs**.\\n\\n> Note: I originally planned to wait until my pull request was merged before publishing this article, but as the review is taking longer than expected, I\u2019ve decided to share my experience now.\\n\\n## The Issue\\n\\nTo get started with the project, I selected a GitHub issue proposed by the core maintainers:\\n[Issue #869 \u2013 Implement Fast CIOS for Montgomery Modular Multiplication](https://github.com/ethereum/evmone/issues/869).\\nBefore diving into the implementation, I took time to understand what Montgomery modular multiplication is.\\n\\n### Montgomery Modular Multiplication\\n\\nMontgomery modular multiplication is an efficient method for computing `\ud835\udc4e\u22c5\ud835\udc4f mod \ud835\udc41` without explicitly performing expensive division operations. Introduced by Peter Montgomery in 1985, it is especially useful in cryptographic algorithms like **RSA** and **elliptic curve cryptography**, where many modular multiplications must be performed rapidly.\\n\\nInstead of working directly with standard modular arithmetic, Montgomery\'s method transforms numbers into a special form called Montgomery representation, defined as: \\n\\n```\\n\ud835\udc4e~ = \ud835\udc4e\u22c5\ud835\udc45 mod \ud835\udc41\\n``` \\n\\nwhere \ud835\udc45 is a power of 2 greater than \ud835\udc41, usually chosen as \ud835\udc45=2^\ud835\udc58 for some integer \ud835\udc58, so arithmetic modulo \ud835\udc45 becomes simple bit masking.\\n\\nThe Montgomery product of `\ud835\udc4e~` and `\ud835\udc4f~` is computed as:\\n\\n```\\nMont(\ud835\udc4e~, \ud835\udc4f~) = \ud835\udc4e~ \u22c5 \ud835\udc4f~ \u22c5 \ud835\udc45^(-1) mod \ud835\udc41\\n```\\n\\nThis yields a result still in Montgomery form. To convert back to the standard representation, a final Montgomery multiplication with 1 is performed.\\n\\nThe advantage is that division by \ud835\udc41 is avoided. Instead, the method relies on a series of additions, multiplications, and bit shifts, which are much faster on modern hardware. The \u201cCIOS\u201d (Coarsely Integrated Operand Scanning) method is a particular way of organizing the loop structure of the multiplication to balance performance and code simplicity.\\n\\n### Fast CIOS Optimization\\n\\nIn 2022, the paper [EdMSM: Multi-Scalar Multiplication for SNARKs and Faster Montgomery Multiplication](https://eprint.iacr.org/2022/1400.pdf) by *Gautam Botrel* and *Youssef El Housni* introduced a way to optimize the **CIOS variant of the Montgomery modular multiplication algorithm**. The paper showed that if the most significant bit of the word used in intermediate operations is not required (meaning that all variables stay below `word_size - 1`), then it\u2019s possible to skip some operations, leading to faster average execution.\\n\\nThis assumption often holds true in practice, making this optimization a worthwhile improvement in many cases.\\n\\n## Implementation\\n\\nThe process of resolving the issue involved several steps. The first was to understand the structure and conventions of the EVMOne codebase, and to ensure that I could compile and run the project locally.\\n\\n![Project Build](/img/evmone_build.png)\\n> Build process for EVMOne.\\n\\nOnce everything was working and all tests passed, I focused on the core of the contribution: the Montgomery algorithm. To fully grasp the optimization, I read several academic papers that detailed the various implementation strategies and explained why the CIOS variant has the lowest computational complexity.\\n\\n![Montgomery Algorithms Variants](/img/emvone_articles.png)\\n> Different variants of the Montgomery algorithm. Source: [High-Speed Algorithms & Architectures For Number-Theoretic Cryptosystems](https://www.microsoft.com/en-us/research/wp-content/uploads/1998/06/97Acar.pdf) by *Tolga Acar*.\\n\\nNext, I located the existing Montgomery implementation within EVMOne and compared it with the optimized CIOS variant described in the research paper.\\n\\n![Montgomery Implementation Comparison](/img/evmone_compare.png)\\n> Comparison of the Montgomery implementation in EVMOne with the optimized CIOS variant pseudocode (from *EdMSM: Multi-Scalar Multiplication for SNARKs and Faster Montgomery Multiplication*).\\n\\nFinally, I made the necessary code changes to align the implementation with the fast CIOS variant described in the paper:\\n\\n```cpp\\n/// Performs a Montgomery modular multiplication.\\nconstexpr UintT mul(const UintT& x, const UintT& y) const noexcept\\n{\\n    constexpr uint64_t most_significant_mod_word_limit{\\n        std::numeric_limits<uint64_t>::max() >> 1};\\n    constexpr auto S = UintT::num_words;  \\n    intx::uint<UintT::num_bits + 64> t;\\n    for (size_t i = 0; i != S; ++i)\\n    {\\n        uint64_t c = 0;\\n        for (size_t j = 0; j != S; ++j)\\n            std::tie(c, t[j]) = addmul(t[j], x[j], y[i], c);\\n   \\n        uint64_t carry = 0;\\n        if (mod[S - 1] < most_significant_mod_word_limit)\\n        { // New branch for Fast CIOS\\n            carry = c;\\n        }\\n        else\\n        { \\n            auto tmp = intx::addc(t[S], c);\\n            t[S] = tmp.value;\\n            carry = tmp.carry;\\n        }\\n        const auto m = t[0] * m_mod_inv;\\n        std::tie(c, std::ignore) = addmul(t[0], m, mod[0], 0);\\n        for (size_t j = 1; j != S; ++j)\\n            std::tie(c, t[j - 1]) = addmul(t[j], m, mod[j], c);\\n\\n        uint64_t carry = 0;\\n        if (mod[S - 1] < most_significant_mod_word_limit)\\n        { // New branch for Fast CIOS\\n            t[S - 1] = carry + c;\\n        }\\n        else\\n        {\\n            auto tmp = intx::addc(t[S], c);\\n            t[S - 1] = tmp.value;\\n            t[S] = carry + tmp.carry;\\n        }\\n    }\\n    if (t >= mod)\\n        t -= mod;\\n    return static_cast<UintT>(t);\\n}\\n```\\n\\nAs you can see, the modifications are relatively minor, consisting mostly of conditional branches that help eliminate unnecessary operations.\\n\\n> \u26a0\ufe0f Please note: these changes have not yet been reviewed and may contain errors.\\n\\nAfter a few trials, all unit tests passed, and I was able to run benchmarks on my own machine to evaluate the performance impact.\\n\\n```\\nComparing cios_classic.json to cios_improved.json\\nBenchmark                                               Time             CPU      Time Old      Time New       CPU Old       CPU New\\nevmmax_mul<uint256, bn254>_median                    -0.1529         -0.1529            27            23            27            23\\nevmmax_mul<uint256, secp256k1>_median                +0.0059         +0.0058            28            28            28            28\\n```\\n\\nThe results showed a consistent 15% performance improvement on the `bn254` test cases, which aligns with the expectations set by the research.\\nIn my most recent comment on the pull request, I mentioned being unsure why two conditional branches performed better than one. Upon further reflection, I believe this may be due to the use of `constexpr`, which allows the compiler to eliminate unused branches at compile-time. Having only one conditional branch introduce more instructions and might prevent such optimizations.\\n\\nYou can find the pull request [here](https://github.com/ethereum/evmone/pull/1009).\\n\\n## Conclusion\\n\\nAs of writing this article, my pull request has not yet been reviewed. Even though the code changes are minimal and early benchmarks support the claims made in the paper, I can\u2019t be fully certain that the proposed implementation will be accepted.\\n\\nThis experience has been extremely valuable \u2014 it taught me more about the mathematical tools involved in elliptic curve cryptography and the implementation of the Montgomery algorithm in high-performance environments.\\n\\nI hope this article helped you learn something new as well, and that you enjoyed reading it. See you next time!"},{"id":"opal","metadata":{"permalink":"/clementjuventin-blog/opal","source":"@site/blog/2024-09-15-opal.md","title":"Opal - Yield DApp built on top of Balancer V2","description":"In September 2023, I was invited to lend a hand on an ambitious DeFi project built on Ethereum called Opal. What started as a casual contribution soon evolved into a core team membership. This is the story of how we built and launched the platform.","date":"2024-09-15T00:00:00.000Z","tags":[{"inline":false,"label":"Opal","permalink":"/clementjuventin-blog/tags/opal","description":"Opal articles"},{"inline":false,"label":"Story","permalink":"/clementjuventin-blog/tags/story","description":"Story about my life"},{"inline":false,"label":"Web3","permalink":"/clementjuventin-blog/tags/web3","description":"Web3 related projects"}],"readingTime":5.07,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"opal","title":"Opal - Yield DApp built on top of Balancer V2","authors":"clementjuventin","tags":["opal","story","web3"]},"unlisted":false,"prevItem":{"title":"My first Open Source Contribution in C++ - EVMOne","permalink":"/clementjuventin-blog/contribution-to-open-source-projects-evmone"},"nextItem":{"title":"Tackling Rate Limiting, One of My First Challenges at Cede Labs","permalink":"/clementjuventin-blog/request-throttler"}},"content":"In September 2023, I was invited to lend a hand on an ambitious DeFi project built on **Ethereum** called **Opal**. What started as a casual contribution soon evolved into a **core team membership**. This is the story of how we built and launched the platform.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Opal Cover](/img/opal_cover.jpeg)\\n\\n## Opal Explained\\n\\nOpal is a **DeFi** protocol rooted in the Ethereum ecosystem, specifically leveraging [Balancer V2](https://balancer.fi/) and [Aura Finance](https://aura.finance/). Its core mission is to offer a simple financial system with dynamic yield-generating strategies. The centerpiece of the protocol is the **Omnipool**: a single-asset liquidity pool that dynamically distributes and rebalances liquidity across multiple pools.\\n\\nLiquidity allocation is governed by a community voting mechanism, powered by the **GEM** token, which represents the protocol\'s decision-making authority.\\n\\nOpal draws heavy inspiration from [**Conic Finance**](https://conic.finance/), which operates within the Curve ecosystem. For more on the financial mechanics, I\u2019ll defer to the experts \u2014 I\'m not the one designing the protocol financial logic, but rather the one helping make it a reality. To understand the intricacies, visit [our documentation](https://docs.opaldefi.xyz/) and appreciate this flywheel being as beautiful as it is hard to understand.\\n\\n![Opal Flywheel](/img/opal_flywheel.png)\\n> The Opal Flywheel. Source: [Opal Documentation](https://docs.opaldefi.xyz/).\\n\\n## The team\\n\\nOur team consisted of four developers and two business/vision managers. We set out to build a complete protocol, production-ready and capable of handling potentially millions in value.\\n\\nThough I lacked experience, I made up for it with **strong fundamentals** and **relentless motivation**. That\u2019s how my Opal journey began \u2014 a bold, real-world challenge I was eager to take on.\\n\\n## Development Journey\\n\\nOver several months, we shaped the protocol and defined the requirements for the first version. I was assigned to the **Reward Manager**, a crucial module responsible for distributing rewards from the Omnipools to users. It was a tricky component, and the initial team had struggled with it, which is why I was brought on board.\\n\\nTo be honest, I struggled too. I\u2019ll dive into the technical and mathematical complexity of this module in a later post.\\n\\nAt this stage, Opal was still a side project for everyone, so the relatively slow development pace suited us. But that was about to change.\\n\\n## Fundraising: Private & Public Rounds\\n\\nWhile development was underway, the business team was busy pitching to investors and organizing a private seed round. I wasn\u2019t deeply involved in this phase, but word was that it went well \u2014 we secured commitments without giving up too many tokens, and **early signs from the market were encouraging**.\\n\\nOur [Twitter](https://x.com/opaldefi) and Discord communities started **gaining traction**, adding pressure on the dev team to finalize the protocol.\\n\\nThe funds raised during the private seed were used to ramp up marketing: promo videos, contests, influencer campaigns \u2014 all in preparation for the public round.\\n\\nThe public raise was hosted on [Fjord Foundry](https://app.fjordfoundry.com/token-sales/0x1F272Ab2BDc512cb59e7b49485eFE16d2d7F9ffa), a platform tailored for such events. The response was phenomenal \u2014 we raised $1.58 million. I could hardly believe it myself.\\n\\n![Opal fundraising metrics](/img/opal_fundraise.png)\\n> Opal fundraising metrics. Source: [Fjord Foundry](https://app.fjordfoundry.com/token-sales/0x1F272Ab2BDc512cb59e7b49485eFE16d2d7F9ffa).\\n\\n## Delivering on Expectations \\n\\nWith a successful raise behind us, the pressure was on. Development needed to accelerate. Looking back, this period was rough \u2014 we clearly underestimated the time required to finalize the product.\\n\\nWe went from working 4\u20136 hours a week to over 20 hours per week, all while juggling full-time jobs. We rushed through two audits, each with just five days to fix reported issues. The first audit revealed more flaws than I expected, which was nerve-racking. But if you\'re in a similar situation \u2014 **don\u2019t panic**. It\'s normal, and you\u2019ll likely go through more audits to patch things up.\\n\\nThe machine was running at full speed. We were in the final stretch.\\n\\n## Opal in Production\\n\\nThey say childbirth is one of the most painful human experiences. I won\u2019t compare directly, but launching Opal felt like a drawn-out labor with complications.\\n\\nWhat was supposed to take hours ended up taking a full week due to last-minute issues. One integration test was producing inexplicable results. Exhaustion and stress didn\u2019t help. We delayed the launch to debug the issue, which turned out to be a decimal misconfiguration. Not catastrophic, but enough to **shake our confidence** just hours before going live.\\n\\nStill, we launched. TVL started rising, eventually hitting **$8 million\u2014a historic high**.\\n\\n## The Downside\\n\\nPost-launch, Opal faced criticism. The community expected a more polished product with more features. We had delivered everything planned for V1, so I was surprised. But it may have been more about market conditions and a separate incident with MEV Capital.\\n\\nMEV Capital, one of our biggest backers, had contributed ~80% of our TVL. They decided to withdraw a large amount of the protocol\'s liquidity without warning. Unfortunately, they encountered a bug in the DApp\'s slippage logic on one of the Omnipools. Without simulating the transaction or using a MEV blocker, they suffered an instant and irreversible **six-figure MEV loss**.\\n\\nTensions rose when the Opal team chose not to assume responsibility. That day, our TVL dropped significantly, and the protocol\'s reputation took a major hit.\\n\\nIt\'s hard to accept that the future of the protocol hinged on a small, avoidable mistake\u2014something that could\u2019ve been prevented in countless ways, both on our side and MEV Capital\u2019s. All I can say is: it happened, and it cost us dearly.\\n\\n## Redemption Arc\\n\\nIn the months that followed, we managed to rebuild some momentum. We secured a grant from Balancer, added new features, and brought TVL back to around $600K.\\n\\nWe\'re currently trying to re-energize the project with an airdrop campaign, community incentives, and feature updates.\\n\\nThe project still suffers from its damaged reputation, and the omnipool model has lost some steam \u2014 especially after Conic Finance started to talk about shutting down their protocol.\\n\\nBut we\'re still optimistic. Our product has been battle-tested, and we remain committed to improving and promoting it. With continued effort, Opal might just win people over again."},{"id":"request-throttler","metadata":{"permalink":"/clementjuventin-blog/request-throttler","source":"@site/blog/2024-04-14-request-throttler.md","title":"Tackling Rate Limiting, One of My First Challenges at Cede Labs","description":"Cede Labs\u2019s SDK is designed to support any number of centralized exchange accounts, allowing applications or users to query data from several CEXs seamlessly. However, this flexibility exposed a serious issue: API rate limits.","date":"2024-04-14T00:00:00.000Z","tags":[{"inline":false,"label":"Cede Labs","permalink":"/clementjuventin-blog/tags/cede-labs","description":"Cede Labs articles"},{"inline":false,"label":"Technical","permalink":"/clementjuventin-blog/tags/technical","description":"Technical articles"}],"readingTime":4.29,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"request-throttler","title":"Tackling Rate Limiting, One of My First Challenges at Cede Labs","authors":"clementjuventin","tags":["cede-labs","technical"]},"unlisted":false,"prevItem":{"title":"Opal - Yield DApp built on top of Balancer V2","permalink":"/clementjuventin-blog/opal"},"nextItem":{"title":"I\'m Joining Cede Labs as a Full-Stack Developer!","permalink":"/clementjuventin-blog/welcome-to-cede-labs"}},"content":"Cede Labs\u2019s SDK is designed to support any number of centralized exchange accounts, allowing applications or users to query data from several CEXs seamlessly. However, this flexibility exposed a serious issue: **API rate limits**.\\n\\n\x3c!-- truncate --\x3e\\n\\nOver the past few weeks at **Cede Labs**, I\u2019ve been diving deep into the codebase, getting familiar with the SDK, and preparing myself for the complex technical challenges ahead. One of the first major problems I encountered \u2014 and helped solve \u2014 was **rate limiting across multiple centralized exchanges (CEXs)**.\\n\\n## The Problem: Managing Rate Limits in a Multi-CEX World\\n\\nCede Labs\u2019 SDK is designed to support any number of centralized exchange accounts, allowing applications or users to query data from several CEXs seamlessly. However, this flexibility exposed a serious issue: **API rate limits**.\\n\\nThe tricky part is that **rate limiting strategies vary widely between exchanges**. Some enforce limits by:\\n\\n- IP address\\n- API key\\n- Master/sub-account hierarchies\\n- Or even combinations of all three\\n\\nWe observed this firsthand through our **Chrome extension**, which integrates the SDK to interact with multiple exchanges. With many CEX accounts active simultaneously, we were quickly exhausting rate limits, resulting in **slowness, API errors, and buggy behavior**.\\n\\nClearly, we needed a robust, centralized system to handle API throttling across all exchange instances.\\n\\n## The Solution: Building a Shared Request Throttler\\n\\nOur SDK uses **CCXT**, a widely adopted library that simplifies communication with CEX APIs. CCXT conveniently includes built-in metadata about rate limits \u2014 including cost calculations and endpoint-specific constraints \u2014 which became the foundation for our fix.\\n\\nI built a component called the **Request Throttler** to manage all outgoing requests. This module:\\n\\n- Runs a continuous loop, processing a queue of API requests\\n- Delays or batches calls based on calculated rate limits\\n- Shares throttling logic between all exchange instances tied to the same CEX\\n- Supports custom priorities to fast-track high-importance requests\\n\\n## The Code: A Deep Dive into the Request Throttler\\n\\nBelow is a simplified version of the throttler\'s main loop. For readability, I\u2019ve omitted queue-empty handling (i.e., when `this.getNext()` returns `null`).\\n\\n```typescript\\nasync loop () {\\n    let lastTimestamp = now ();\\n    while (this.running) {\\n        const { resolver, cost, rejecter, timestamp, expireInterval } = this.getNext ();\\n\\n        if (this.tokens >= 0) {\\n            if (timestamp + expireInterval < now ()) {\\n                rejecter (\'Request expired\');\\n            } else {\\n                this.tokens -= cost;\\n                resolver ();\\n            }\\n            await Promise.resolve ();\\n        } else {\\n            await sleep (this.delay);\\n            const current = now ();\\n            const elapsed = current - lastTimestamp;\\n            lastTimestamp = current;\\n            const tokens = this.tokens + this.refillRate * elapsed;\\n            this.tokens = Math.min (tokens, this.capacity);\\n        }\\n    }\\n}\\n```\\n\\n#### Key Concepts\\n- `this.tokens` represents the available request \\"budget\\". It\'s reduced by the `cost` of each request.\\n- If `this.tokens` is below 0, the system will pause (`sleep`) and **refill** tokens based on elapsed time and a configured **refill rate**.\\n- If tokens are available (i.e., `>= 0`), the request is executed **only if it hasn\'t expired** (based on `timestamp + expireInterval`).\\n- The line `await Promise.resolve();` is a **context switch** \u2014 it gives other asynchronous tasks a chance to execute, which helps avoid blocking the event loop in JavaScript.\\n- **Token refill logic** happens after the delay and simulates a \\"leaky bucket\\" or \\"token bucket\\" algorithm, a common strategy in rate-limiting systems.\\n\\nThis loop ensures that all API requests respect the configured rate limits by dynamically adjusting based on real-time usage and availability.\\n\\n## Prioritization and Background Work\\n\\nIn practice, our SDK performs a lot of **background tasks**, such as:\\n\\n- Balance fetching\\n- Historical trade retrieval\\n- Status checks\\n\\nThese operations are necessary but can be expensive in terms of API cost. So, I implemented a **priority system**: important requests (like user-triggered actions) are associated to high priority while background tasks are associated to low priority.\\n\\nThis ensures faster user experiences without breaking rate limits or starving essential tasks.\\n\\n## The Hard Part: Rate Limit Discovery\\n\\nThe most difficult aspect wasn\u2019t writing the throttler logic itself \u2014 it was understanding and standardizing **how each exchange handles rate limits**.\\n\\n- Many rate-limiting rules are hidden, sometimes undocumented and in worse cases not aligned with the official documentation\\n- CCXT\u2019s built-in cost definitions were often outdated\\n- Some CEXs change behavior depending on the account type or endpoint\\n\\nI had to manually test, validate, and correct the API cost values for several exchanges, updating the SDK accordingly.\\n\\nIt also took careful work to **share a throttler** across multiple CEX instances (i.e., multiple ccxt objects) since each has unique constraints, such as caching behavior or read/write permission separation.\\n\\nFor context:\\n> A CEX instance in our system is a `ccxt` object tied to a specific API key and access rights. Each instance needs to respect its rate limits but also coordinate with other instances for the same exchange.\\n\\nThat\u2019s where the Request Throttler comes in \u2014 it sits above the instances and coordinates them like air traffic control.\\n\\n## Looking Ahead\\n\\nThis was a great first challenge at Cede Labs. It pushed me to go beyond just writing code \u2014 I had to understand CEXs inside out, map inconsistencies, and design a system that works under real-world constraints.\\n\\nThere\u2019s still a lot to build and while this system highly reduced our rate limit issues, there is still a lot of improvements to make to reach our goals.\\n\\nI\u2019m proud of how this first technical hurdle turned into a solid foundation for more scalable and reliable SDK behavior.\\n\\nMore to come soon!"},{"id":"welcome-to-cede-labs","metadata":{"permalink":"/clementjuventin-blog/welcome-to-cede-labs","source":"@site/blog/2023-03-02-welcome-to-cede-labs.md","title":"I\'m Joining Cede Labs as a Full-Stack Developer!","description":"I\u2019m thrilled to announce that I\u2019m joining Cede Labs as a Full-Stack Developer!","date":"2023-03-02T00:00:00.000Z","tags":[{"inline":false,"label":"Cede Labs","permalink":"/clementjuventin-blog/tags/cede-labs","description":"Cede Labs articles"},{"inline":false,"label":"Story","permalink":"/clementjuventin-blog/tags/story","description":"Story about my life"},{"inline":false,"label":"Web3","permalink":"/clementjuventin-blog/tags/web3","description":"Web3 related projects"}],"readingTime":1.73,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"welcome-to-cede-labs","title":"I\'m Joining Cede Labs as a Full-Stack Developer!","authors":"clementjuventin","tags":["cede-labs","story","web3"]},"unlisted":false,"prevItem":{"title":"Tackling Rate Limiting, One of My First Challenges at Cede Labs","permalink":"/clementjuventin-blog/request-throttler"}},"content":"I\u2019m thrilled to announce that **I\u2019m joining Cede Labs as a Full-Stack Developer**!\\n\\n\x3c!-- truncate --\x3e\\n\\nCede Labs is a Paris-based French startup building the missing bridge between **centralized exchanges (CEXs)** and the **decentralized finance (DeFi) ecosystem**. Their mission is to create seamless connectivity between Web2 and Web3 financial systems \u2014 and I\u2019m excited to become a part of that journey.\\n\\n## What Cede Labs does\\n\\nAt its core, Cede Labs offers a powerful Software Development Kit (SDK) that enables developers to interact with over 10 major centralized exchanges (*Binance, Bybit, Coinbase, etc.*) and more than 1,000 supported blockchains. With this SDK, developers can:\\n\\n- Retrieve account and trading history\\n- Fetch balances\\n- Execute trades\\n- Deposit and withdraw assets\\n\\nBut that\'s not all. The team is also building a Chrome extension wallet that integrates directly with their SDK, providing a smooth and user-friendly interface. It enables users to interact with DeFi applications just like they would with a standard Web3 wallet like MetaMask \u2014 but with deeper CEX capabilities under the hood.\\n\\n## Why this Role Excites Me\\n\\nThis opportunity marks my **official entry into the Web3 industry**, something I\'ve been eager to pursue. Even more exciting, it allows me to continue applying my experience in **traditional finance development**, combining the best of both worlds.\\n\\nThe potential use cases for the SDK are vast, and the roadmap ahead is full of innovation \u2014 from enhanced trading features like **futures support** to **onboarding new exchanges**. There\'s a lot to build, and I\'m thrilled to contribute.\\n\\n## What I\'ll Be Working On\\n\\nAs a full-stack developer at Cede Labs, my role spans across both frontend and backend, including:\\n\\n- Developing the Chrome extension UI and adding key features\\n- Contributing to the SDK, enhancing its capabilities and exchange support\\n- Building and maintaining a NestJS API\\n- Managing infrastructure deployments on AWS using Terraform\\n\\n## Looking Ahead\\n\\nThere\u2019s a ton of exciting work ahead, and I can\u2019t wait to share what we\u2019re building at Cede Labs. I\u2019m looking forward to learning from the brilliant team and bringing my own contributions to the table. Stay tuned \u2014 more updates coming soon!"}]}}')}}]);