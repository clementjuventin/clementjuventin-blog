"use strict";(self.webpackChunkpersonal_website=self.webpackChunkpersonal_website||[]).push([[890],{161:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"building-an-arbitrager","metadata":{"permalink":"/clementjuventin-blog/building-an-arbitrager","source":"@site/blog/2025-06-03-building-an-arbitrager.md","title":"Building an Arbitrage Bot","description":"I\'ve been fascinated by MEV (Maximal Extractable Value) for a while now, and over the past few weeks, I\u2019ve started building my own arbitrage bot. It\u2019s been a challenging yet insightful journey, and I\u2019d like to share my experience so far.","date":"2025-06-03T00:00:00.000Z","tags":[],"readingTime":11.9,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"building-an-arbitrager","title":"Building an Arbitrage Bot","authors":"clementjuventin","tags":[]},"unlisted":false,"nextItem":{"title":"From A Nooby Degen to a Bloodthirsty Sniper","permalink":"/clementjuventin-blog/degen-to-sniper"}},"content":"I\'ve been fascinated by MEV (Maximal Extractable Value) for a while now, and over the past few weeks, I\u2019ve started building my own arbitrage bot. It\u2019s been a challenging yet insightful journey, and I\u2019d like to share my experience so far.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is MEV?\\n\\nPeople trade funds, people watch them trading funds, people found ways to make profits based on funds being traded.\\nTo be honest, this is not today\'s topic, and it\'s a very vast field. To learn more, [this article may help you](https://www.coingecko.com/learn/what-is-mev-maximal-extractable-value-crypto).\\n\\n## Starting with the Basics\\n\\nTo begin this adventure, I decided to explore the implementation of a **CowSwap solver**.\\n\\nFor those unfamiliar, [CowSwap](https://cow.fi/cow-protocol) is an off-chain order book where solvers **compete to submit the most efficient on-chain settlement**. Since all solvers use the same underlying liquidity sources, the competition lies less in speed or on-chain monitoring tricks, and more in **algorithmic quality** (from what I understand).\\n\\nI figured this would be a great way to get hands-on experience with arbitrage using real-world tools, while also learning from the CowSwap ecosystem itself.\\n\\nUnfortunately, that turned out to be a bit optimistic. The competitiveness of an arbitrage system is deeply tied to its performance \u2014 which means that sharing strategies often **undermines profitability**. So, I quickly realized that there wouldn\u2019t be much help available when it came to solving the core problems.\\n\\nThat said, I followed CowSwap\u2019s documentation closely and was able to get the full local stack up and running, including syncing with the live order book.\\n\\nI chose to implement my solver in **C++** \u2014 for once, it wasn\u2019t overkill. I was surprised to find I was already familiar with many of the liquidity models: **Uniswap V2/V3**, **Balancer\u2019s stable and weighted pools**, and a few others. The remaining pool types seemed negligible.\\n\\n### Then the Real Challenge Hit\\n\\nThe complexity came when I had to actually **write the algorithm** \u2014 and I had no clear direction. Neither the documentation nor any online resource offered much guidance.\\n\\nAn ideal solver algorithm would likely involve:\\n\\n- **Graph theory**, to identify optimal trade paths\\n- **Game theory**, to balance solver profitability with order competitiveness\\n- **Mathematical modeling**, to simulate off-chain liquidity movements\\n- Handling **limit orders**, **partial orders**, and other edge cases\\n\\nIn short, this problem is likely **NP-complete**, requiring advanced algorithms and a deep mathematical foundation. I had just thrown myself headfirst into the deep end of arbitrage complexity.\\n\\n## Can You Still Get Results Without a PhD Team?\\n\\nThat\u2019s the question I began asking myself. Is it possible to **simplify the problem** and/or **reduce the level of competition**? It was clear that I had no chance to compete with the big players, even after a few months of development.\\n\\nOne idea came to mind: target a **newer blockchain** \u2014 one that\u2019s less mature in terms of ecosystem and developer tooling. That would mean less competition, and maybe a better learning ground.\\n\\n### Enter HyperEVM\\n\\nMy search led me to **HyperEVM**, a relatively new EVM-compatible blockchain developed by [**Hyperliquid**](https://hyperliquid.gitbook.io/hyperliquid-docs). It felt like stepping into the Wild West.\\n\\nWhy **HyperEVM** was promising:\\n\\n- **Low gas fees**\\n- **Modest but exploitable trading volume**\\n- **EVM compatibility** (making integration much easier)\\n- **Fewer active arbitrage bots**, due to its early-stage ecosystem\\n\\nSo I began roughing out a system that could serve as an arbitrage bot on this new chain. I\u2019ll describe the different directions I explored to improve the bot\u2019s profitability. I experimented as much as I could and tried to learn by doing, testing every idea that seemed viable.\\n\\n## The Adventure Begins\\n\\nChoosing an EVM-compatible chain seemed like a great idea \u2014 in theory, it would allow me to port this system to any other EVM-compatible chain later on. In practice, though, things turned out to be more complicated due to protocols specificities.\\n\\n### The First Proof of Concept\\n\\nTo build my proof of concept (PoC), I needed a few key components:\\n\\n- Fast access to on-chain data\\n- Off-chain liquidity simulation for Uniswap-like pools\\n- The ability to send on-chain transactions with protection against slippage\\n\\nI started with a **QuickNode RPC**, but quickly hit the 200 requests/sec limit. That capped me at monitoring roughly 30 pools during testing.\\n\\nI integrated [**Hyperswap**](https://hyperswap.exchange/) and [**Kittenswap**](https://kittenswap.finance/), the two most active DEXs on **HyperEVM**.\\n\\nMy first approach, somewhat naive, was to look for arbitrage loops that start and end with the same token. That way, there\'s no need to rely on price approximations (e.g., A \u2192 B \u2192 A or A \u2192 B \u2192 C \u2192 A). I deployed a smart contract capable of executing multiple swaps and reverting if the final balance was lower than the initial one. This ensure that the bot will never spend more than it earns.\\n\\n```solidity\\nfunction execute(\\n    Trade[] calldata trades,\\n    uint256 amountIn,\\n    address startToken\\n) external payable returns (uint256) {\\n    uint256 balanceBefore = IERC20(startToken).balanceOf(address(this));\\n\\n    // ... perform trades ...\\n\\n    uint256 balanceAfter = IERC20(startToken).balanceOf(address(this));\\n    // This line will revert all the previous operations and stop the execution\\n    require(balanceAfter > balanceBefore, \\"Arbitrage failed\\");\\n\\n    return balanceAfter - balanceBefore;\\n}\\n```\\n\\nIn TypeScript, I implemented functions to compute expected token outputs for Uniswap V2 and approximated Uniswap V3 swaps \u2014 V3 is more complex due to its tick system while V2 is pretty straightforward.\\n\\nTo detect arbitrage opportunities, I wrote a simple brute force algorithm that computes the best arbitrage path for a given input amount.\\n\\nI decided to hardcode the input amount to 1 HYPE, and to ignore the gas cost of the transaction because it was extremely low at the time.\\n\\n\\n### The First Result\\n\\nWith everything wired up, I ran the bot, stepped away for 20 minutes\u2026 and came back to see my **first profitable transaction \u2014 about $0.30** at the time.\\n\\n![First Arbitrage Transaction](/img/first_arbitrage_tx.jpg)\\n> My first arbitrage transaction on HyperEVM. Source [Purrsec](https://purrsec.com/tx/0x710e7c9dd53dad1aa3cb15b3225eb1b777a3b230a750e4a36f1d4eb9de17e87e).\\n\\nIt was a small gain, but a huge morale boost. At that point, I believed I was close to a viable system. With a dedicated node and estimated infrastructure costs around $100/month, profitability seemed achievable.\\n\\n**Spoiler**: I still had a long road ahead. Competition would ramp up quickly in the days to come. But in hindsight, that initial optimism was crucial \u2014 if I had only focused on the months of work and potential financial losses ahead, I might have stopped right there.\\n\\n## Moving Toward Production\\n\\nSo far, here\'s a diagram of my architecture \u2014 each module is explained below.\\n\\n![Architecture](/img/arbitrager_archi.png)\\n> Colors are just here to make things prettier \ud83d\udc4c\u200b.\\n\\n### HyperEVM Custom Node\\n\\nMy first step toward production was setting up **a local node**. This gave me full on-chain access with low latency and no rate limits. I didn\'t follow the advice from the documentation, which recommended using a node in Japan to reduce latency issues, even though it was cheaper in the US. I eventually switched and ended up losing several days syncing the node with a machine in Tokyo due to latency issues.\\n\\n### Cold data Indexer\\n\\nI needed a way to cache persistent data (e.g., token names, decimals, pool tokens). So I wrote an **indexer that stores data in Redis**, refreshed periodically.\\n\\n### Solvers\\n\\nEach solver receives a request from the driver and solves a graph to find arbitrage paths. Initially, this logic was in **JavaScript** for rapid prototyping. Later, I migrated to **C++**, which was 80x faster on average \u2014 crucial when tracking many pools. It is the same brute force algorithm I used in the PoC.\\n\\n```cpp\\ninline void find_best_arbitrage_cycle(\\n    const std::vector<std::vector<const Api::LiquidityPoolTradePath *>> &all_cycles,\\n    const Api::uint256 amount_in, \\n    Api::uint256 &best_amount_out,\\n    std::vector<const Api::LiquidityPoolTradePath *> &best_cycle\\n    ) {\\n  for (const auto &cycle : all_cycles) {\\n    try {\\n      const Api::uint256 amount_out = compute_cycle_amount_out(cycle, amount_in);\\n\\n      if (amount_out > best_amount_out) {\\n        best_amount_out = amount_out;\\n        best_cycle = cycle;\\n      }\\n\\n    } catch (Math::InvalidLiquidityStateError &e) {\\n      continue;\\n    } catch (const std::exception &e) {\\n      std::cout << \\"Error: \\" << e.what() << std::endl;\\n    }\\n  }\\n}\\n```\\n\\nNote that I was heavily inspired by CowSwap. It was important for me to design the system with multiple solvers from the start, as this would allow for horizontal scaling of the solving process, as well as safe comparison of different algorithms \u2014 since each solution is evaluated and put in competition with the others.\\n\\n### Monitoring \\nTo monitor performance, I built a metrics indexer that feeds **Prometheus**, which powers **Grafana** dashboards.\\n\\nTo be honest, I under-invested here. I\'m still missing key insights and can\u2019t precisely track profit/loss over time. The idea was to compare strategies and versions of the arbitrage engine, but I\u2019ve mostly been busy building essential features.\\n\\nYou can see in the following screenshot that this allows me to visualize the increase in the contract\'s balances (due to arbitrage activity) and the amount of HYPE available to cover transaction fees for the wallets responsible for calling the arbitrageur.\\n\\n![Grafana dashboard](/img/arbitrager_dashboard.png)\\n> Grafana dashboard (with a bug regarding BTC decimals).\\n\\n### Driver\\n\\nI built a central coordination module called the **Driver** (also inspired by CowSwap\u2019s architecture). It\u2019s responsible for:\\n\\n- High-frequency data collection\\n- Structuring the data into a standard request format\\n- Sending requests to all available solvers\\n- Collecting and evaluating responses within a strict timeout\\n- Comparing the results to determine the best solver and submit the transaction\\n\\nSimulating gas cost and submitting transactions reliably was another challenge. I eventually introduced a dynamic fee deduction, setting aside a portion of profit to cover validator fees and ensure transaction inclusion. I am still not convinced with this approach, but I have no better idea for now.\\n\\nThanks to the strategy of using multiple solvers, I was able to narrow down the list of cryptocurrencies used to initiate arbitrage cycles. This increases the number of opportunities, as it allows for shorter arbitrage loops.\\n\\nFor example, a cycle like A \u2192 B \u2192 C \u2192 B \u2192 A can be shortened to B \u2192 C \u2192 B, saving two transactions if the contract already holds B.\\n\\nResponses are evaluated and filtered \u2014 in the PoC, I hardcoded the input token amount to 1 HYPE, which was suboptimal. Profits vary significantly depending on input size.\\n\\nTo omptimize the input size estimation, I tried different strategies:\\n\\n#### Offchain computation\\n\\nOff-chain calculation is fast and reliable \u2014 if you have accurate models and complete data. For Uniswap V2, the math is simple. For V3, it\u2019s [**evil**](https://uniswapv3book.com/index.html).\\n\\nIn the PoC, I used a rough approximation to avoid tick-level math. To compute real outputs with Uniswap V3\'s SDK, you need **all tick data**, which is extremely resource-intensive \u2014 even with a private node.\\n\\nMy idea was to fetch a subset of ticks because arbitrage transactions are often low-volume. I managed to code the math but the delays were still significant when it came to fetch the data. I finally gave up this approach for the next strategy.\\n\\n#### Onchain computation\\n\\nInstead of fully computing everything off-chain, I opted for a hybrid approach:\\n\\n- Approximate swaps using a constant-tick assumption (ie. Uniswap V3 math approximation)\\n- Collect all promising trade paths\\n- Simulate each candidate on-chain via a custom smart contract (a read-only call)\\n- Use binary search (dichotomy) to find the input amount that maximizes profit\\n\\nThis approach is based on the fact that the data is available, but communication between the driver and the node results in significant latency. Even when both programs run on the same machine, each request still has to go through the full network stack. Since tick retrieval can\'t be properly parallelized, fetch times become a bottleneck.\\n\\nInstead, the node is tasked with simulating a smart contract execution. This way, it uses the same math as the actual contracts (eliminating the risk of errors) and has direct, optimized access to its own data.\\n\\nThis gave me much better results while avoiding heavy off-chain computation. Here is a snippet of the binary search function, it returns the optimal `k` value (ie. the best amount) to maximize the profit:\\n\\n```sol\\nfunction binarySearchOptimalK(\\n    Trade[] calldata trades,\\n    uint256 step,\\n    uint256 balance\\n) internal returns (uint256) {\\n    uint256 low_k = 1;\\n    uint256 high_k = balance / step;\\n    while (low_k < high_k) {\\n        uint256 mid_k = low_k + (high_k - low_k) / 2;\\n        uint256 profitMid = computeTradesProfit(\\n            trades,\\n            mid_k * step\\n        );\\n        uint256 profitNext = computeTradesProfit(\\n            trades,\\n            (mid_k + 1) * step\\n        );\\n        if (profitMid >= profitNext) {\\n            high_k = mid_k;\\n        } else {\\n            low_k = mid_k + 1;\\n        }\\n    }\\n    return low_k;\\n}\\n```\\n\\nAnd this is **costless** because I am **simulating** this call (even though it\'s a write function).\\n\\n### Smart Contract Module\\n\\nMy first smart contract was simple, and it worked well initially.\\nThe main purpose is to prevent the bot to concretely loose funds (e.g. Spend 1 A and obtain 0.98 A).\\n\\nBut as my whole bot system matured, it became clear that the contract was **critical to profitability**.\\n\\nProfit calculation is straightforward: `profit = total_output - total_input - gas_cost`.\\n\\nBut when a transaction fails, you still pay the gas \u2014 often due to a competing bot front-running your trade.\\nThese failed transactions became increasingly costly due to the gas price increasing of a ~500x factor (HyperEVM became more popular).\\n\\n![HyperEVM daily transactions](/img/hyperevm_daily_tx.png)\\n> HyperEVM daily transactions. Source [Purrsec](https://purrsec.com/trending/metrics).\\n\\nTo mitigate this:\\n\\n- I optimized gas usage with basic Solidity gas optimizations\\n- Introduced an early-exit condition before the final balance comparison (`balanceAfter < balanceBefore`)\\n- Tweaked execution to minimize unnecessary operations\\n\\nThis lowered the average cost per trade significantly, but not enough.\\n\\n## Current Results & Next Steps\\n\\nAs of now, the bot is not yet profitable. On average, I recover only ~80% of the input.\\n\\nTo bridge the gap to profitability, I have two main ideas:\\n\\n- Reduce fees further\\n- Use heuristics to detect and avoid unprofitable trades\\n\\nThe second option seems most promising \u2014 I haven\u2019t explored it yet, and it requires a larger dataset to analyze trade outcomes at scale. I\'ll need to analyze all trades performed and hopefully find patterns that reduce the number of failed transactions.\\n\\nAlso, I face a new challenge: **latency**.\\n\\nBlock time on HyperEVM has dropped to under 1 second (2 seconds originally). Because my experiments run from a machine in US, network latency to the HyperEvm sequencers (hosted in Tokyo) is hurting my competitiveness.\\n\\nI\u2019ll need a new setup \u2014 ideally a server located closer to the sequencer region \u2014 to reduce latency and stay in sync.\\n\\n## Final Thoughts\\n\\nThere\u2019s still a long road ahead, but I believe it\u2019s worth it. I\u2019ve learned a ton about arbitrage, on-chain mechanics, and smart contract optimization \u2014 and I\u2019m only getting started."},{"id":"degen-to-sniper","metadata":{"permalink":"/clementjuventin-blog/degen-to-sniper","source":"@site/blog/2025-03-25-degen-to-sniper.md","title":"From A Nooby Degen to a Bloodthirsty Sniper","description":"In this article I\'d like to recount my first steps as a degen and what prompted me to turn to the world of sniping.","date":"2025-03-25T00:00:00.000Z","tags":[],"readingTime":8.57,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"degen-to-sniper","title":"From A Nooby Degen to a Bloodthirsty Sniper","authors":"clementjuventin","tags":[]},"unlisted":false,"prevItem":{"title":"Building an Arbitrage Bot","permalink":"/clementjuventin-blog/building-an-arbitrager"},"nextItem":{"title":"Opal - Yield DApp built on top of Balancer","permalink":"/clementjuventin-blog/opal"}},"content":"In this article I\'d like to recount my first steps as a degen and what prompted me to turn to the world of sniping.\\n\\n\x3c!-- truncate --\x3e\\n\\nUp until recently, I had always kept an eye on the web3 and blockchain space from a distance. I followed the news, attended events and conferences, and got to know many of the key players who set the **foundations and rules of the ecosystem**.\\n\\nBut there was always this one group that felt almost mythical to me \u2014 the **degens**. A close friend of mine was part of what I used to label as the degen community. He belonged to private groups focused on **alpha leaks, technical collaboration, and all sorts of niche financial services**. For a long time, it all seemed a bit unproductive to me from a personal growth standpoint, so I kept my distance.\\n\\nOver time, however, I began to realize that beyond the financial aspect, there was a real sense of connection \u2014 a network that could be nurtured and leveraged as an additional driver for success. To be honnest, I also had more time than ever for this kind of activity.\\n\\nSo I decided to dive in and start my journey as a degen. It turned out to be much more challenging than I had anticipated. I quickly discovered that this world demands an **almost full-time commitment**. It\u2019s a game of **hunting, anticipating, and executing transactions** based on public data. Starting out on **Base** (Coinbase\'s blockchain), I quickly lost my first few hundred dollars. But I knew the potential was there. After a few weeks, I managed to break even and eventually started seeing small profits.\\n\\nAs the weeks passed, I identified some of my key strengths and weaknesses.\\n\\n**Strengths**:\\n\\n- I have a technical background. I can understand the intricate details of projects and deeply master the tools needed to operate effectively in this space.\\n- I have valuable contacts in the industry who\u2019ve generously shared their knowledge and saved me months of trial and error.\\n\\n**Weaknesses**:\\n\\n- I work solo. Despite those contacts, I can\u2019t rely on them indefinitely. In the degen world, data is money. I can\u2019t expect them to constantly feed me information. Also, being a solo operator means I\u2019m limited by time, whereas groups can distribute the workload and share information more efficiently.\\n\\nThat\u2019s when I decided to team up with an old friend who was already well-established in the ecosystem. He was also a dev and had access to valuable information. I, on the other hand, had time and solid technical discipline to support him on various projects.\\n\\nTogether, we began developing a range of analytics and monitoring tools. Without going into too much detail, we built:\\n- AlphaGate monitoring\\n- Twitter activity monitors\\n- Factory contract monitors on Base\\n- Onchain OSINT for Twitter/Warpcaster addresses\\n- Onchain Data analytics tools\\n\\nAll these projects were just the beginning \u2014 a warm-up for something much bigger: diving into **crypto sniping**.\\n\\n## What Is Sniping?\\n\\nBy sniping, I mean setting up a system to **quickly buy a cryptocurrency just before it catches a wave of attention**. Imagine a highly anticipated token is about to launch. The market hasn\u2019t priced it in yet, and for a brief moment, it\u2019s undervalued. A sniper aims to e**xploit that gap** \u2014 buying at launch before the market corrects.\\n\\nThis practice is often frowned upon \u2014 and understandably so. When you buy early with the sole intent to flip for a quick profit, you inevitably hurt retail investors who get in later and might immediately find themselves in the red. Indirectly, this also harms the project itself.\\n\\nBut in the end, the only rule that truly matters is the market. Sniping is a fierce game \u2014 full of traps, failures, and intense competition. Being consistently profitable is just as difficult as in any other degen strategy. I\u2019ve had bots miss the entry entirely while friends manually secured massive gains.\\n\\nIn the coming post, I\u2019ll break down some of the sniping techniques I\u2019ve personally developed \u2014 along with a few wild anecdotes from the field.\\n\\n![Pepe Sniper](/img/sad_pepe_sniper.png)\\n\\n## Sniping Techniques\\n\\nThere are many different sniping techniques, and after watching countless token launches, I can confidently say that **no two are ever the same**. Sometimes projects announce the contract on Twitter, other times on Telegram, through a custom website interface, or via a launchpad. You always need to do some investigation to anticipate which sniping strategy has the highest chance of working.\\n\\nThe two sniping techniques that have brought me the most consistent profits are the following.\\n\\n### Official Claim Sniper\\n\\nWhat I call the *official claim sniper* is a bot that waits for a verified announcement from a trustworthy source before buying the token. Scammers often deploy fake versions of tokens, so without direct confirmation from the project team, jumping in right after deployment is **extremely risky**. Some snipers take that gamble anyway, assuming the potential profits outweigh the possible losses. Personally, I prefer the safer route, even if it means lower profit multipliers \u2014 **risk management comes first**.\\n\\nThis type of bot is relatively simple. It only requires a **notification module** and a **transaction module**. A typical architecture I\u2019ve used involves multiple purchase microservices (one per chain), all exposed through a standard API, along with a separate notification microservice configured per opportunity to monitor data and trigger the buy. Hosting both services on the same machine is important to maximize speed.\\n\\nFor example, you could monitor tweets from a memecoin on Avalanche, extract any EVM address with a regex, and buy on whatever DEX has liquidity for that token.\\n\\nIn theory, it sounds simple. But in practice, **there are many potential points of failure** that can ruin the operation.\\n\\nFirst, the competition is **brutal**. Everyone\u2019s sniper bots are tuned to fire in the same seconds. The first few moments after a hyped token launch usually show a **huge price spike**. That\'s because the earliest snipers are buying aggressively, inflating the price before the market stabilizes. If your bot isn\u2019t fast enough, you\u2019ll end up buying at the top and instantly taking a loss as early buyers dump their tokens. You need to strike a balance: be early enough to profit, but not so aggressive that you pay too much in gas or slippage.\\n\\n![KAIKU/WETH pair on Base](/img/dexscreener.com_KAIKU_WETH_2025-06-06_00-37-34.png)\\n> KAIKU/WETH pair on Base (at launch), one candle = 2s (ie block time). Source: [Dexscreener](https://dexscreener.com/base/0x4c9498a3f36709ee57b1b7c4b440d8481a1b9f79).\\n\\nSecond, the projects themselves have started fighting back. Many view sniping as toxic behavior and implement countermeasures:\\n\\n- Obfuscating the contract address (e.g., inserting extra characters or posting it as an image)\\n- Sharing a link to the DEX liquidity pool instead of the contract directly\\n- Using redirect links to custom sites where the contract is revealed\\n- Deploying a fake token first, then warning the community about it\\n- Enabling extreme transaction taxes for the first few seconds to penalize snipers\\n\\nAll these traps carry real risks \u2014 from simply **missing the opportunity** to **losing your entire investment**. With solid preparation and strategies involving AI or semi-public information feeds, you can sometimes bypass these tricks faster than a human could.\\n\\nA great example of how dangerous this can get is detailed in [this excellent post](https://tactical.deepwaterstudios.xyz/p/anti-sniper-tech-custom-dex), which explains how anti-bot tactics drained massive amounts from automated snipers. We nearly fell victim ourselves during that period. While I was casually skiing down a slope, waiting for a midday launch, my partner was glued to Twitter. He spotted a Discord message revealing the project team\u2019s plan was to bait snipers with a fake contract. On reflex, he shut down our script. One minute later, a fake contract tweet dropped \u2014 and just like that, another sniper lost $75,000.\\n\\nWe were shaken. Even though we weren\u2019t risking that much, it was enough to seriously hurt. Ironically, the competitor immediately re-entered the market with the same amount (what confidence!). In the end, both their bot and ours failed to enter the market, likely due to the a technical hiccup. But the experience was a sobering reminder: even with good tooling and timing, **the risk is never zero**.\\n\\n### Block Zero Sniper\\n\\nThis technique relies on knowledge of the launchpad that will host the token launch, as well as additional details like the deployer address or token ticker. The idea is to deploy a smart contract that sends a flood of buy transactions every block, attempting to snipe within the same block the liquidity pool is created.\\n\\nThis is possible because many launchpads (like Virtuals) use a factory contract that updates a counter or state variable when a new pair is created. By monitoring that state at high frequency, the sniper can instantly detect a new pool and trigger a buy. Ideally, you filter targets using the ticker or deployer to avoid blindly buying into every new token.\\n\\nThis reminds me of an interesting anecdote from Binance Smart Chain. CZ, Binance\u2019s founder, had announced he would reveal the name of his dog at 16:00 UTC \u2014 a seemingly harmless statement that degens took very seriously. Why? Because they knew people would rush to create tokens named after the dog, sparking a frenzy of speculation. One sniper had the brilliant idea to create a smart contract with $10,000, programmed to invest $1,000 into each of the first 10 tokens deployed after 16:00 UTC.\\n\\nYou\u2019ve got to admire that level of confidence \u2014 blindly throwing tens of thousands into what could be worthless tokens. But in this case, it paid off. Tree of the ten tokens was related to CZ\'s post \u2014 BROCOLLI (the dog) \u2014 made that sniper a millionaire in under 3 seconds. WTF bro what if CZ wanted a beer at 16:00 UTC and posted at 16:01?\\n\\n## Conclusion\\n\\nAs you\'ve seen, sniping techniques vary wildly, and **the risks are as real as the rewards**. In practice, we only make profitable trades in about 1 out of every 5 attempts. **Most of the time, nothing happens**.\\n\\nOne of the biggest lessons I\u2019ve learned is that technical sophistication alone doesn\u2019t guarantee success. Often, creativity, originality, and unconventional thinking bring more value than raw code. Always keep an open mind and approach every opportunity like a new challenge \u2014 because in this game, that\'s exactly what it is."},{"id":"opal","metadata":{"permalink":"/clementjuventin-blog/opal","source":"@site/blog/2024-09-15-opal.md","title":"Opal - Yield DApp built on top of Balancer","description":"In September 2023, I was invited to lend a hand on an ambitious DeFi project built on Ethereum called Opal. What started as a casual contribution soon evolved into a core team membership. This is the story of how we built and launched the platform.","date":"2024-09-15T00:00:00.000Z","tags":[],"readingTime":5.07,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"opal","title":"Opal - Yield DApp built on top of Balancer","authors":"clementjuventin","tags":[]},"unlisted":false,"prevItem":{"title":"From A Nooby Degen to a Bloodthirsty Sniper","permalink":"/clementjuventin-blog/degen-to-sniper"},"nextItem":{"title":"Tackling Rate Limiting, One of My First Challenges at Cede Labs","permalink":"/clementjuventin-blog/request-throttler"}},"content":"In September 2023, I was invited to lend a hand on an ambitious DeFi project built on **Ethereum** called **Opal**. What started as a casual contribution soon evolved into a **core team membership**. This is the story of how we built and launched the platform.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Opal Cover](/img/opal_cover.jpeg)\\n\\n## Opal Explained\\n\\nOpal is a **DeFi** protocol rooted in the Ethereum ecosystem, specifically leveraging [Balancer](https://balancer.fi/) and [Aura Finance](https://aura.finance/). Its core mission is to offer a simple financial system with dynamic yield-generating strategies. The centerpiece of the protocol is the **Omnipool**: a single-asset liquidity pool that dynamically distributes and rebalances liquidity across multiple pools.\\n\\nLiquidity allocation is governed by a community voting mechanism, powered by the **GEM** token, which represents the protocol\'s decision-making authority.\\n\\nOpal draws heavy inspiration from [**Conic Finance**](https://conic.finance/), which operates within the Curve ecosystem. For more on the financial mechanics, I\u2019ll defer to the experts \u2014 I\'m not the one designing the protocol financial logic, but rather the one helping make it a reality. To understand the intricacies, visit [our documentation](https://docs.opaldefi.xyz/) and appreciate this flywheel being as beautiful as it is hard to understand.\\n\\n![Opal Flywheel](/img/opal_flywheel.png)\\n> The Opal Flywheel. Source: [Opal Documentation](https://docs.opaldefi.xyz/).\\n\\n## The team\\n\\nOur team consisted of four developers and two business/vision managers. We set out to build a complete protocol, production-ready and capable of handling potentially millions in value.\\n\\nThough I lacked experience, I made up for it with **strong fundamentals** and **relentless motivation**. That\u2019s how my Opal journey began \u2014 a bold, real-world challenge I was eager to take on.\\n\\n## Development Journey\\n\\nOver several months, we shaped the protocol and defined the requirements for the first version. I was assigned to the **Reward Manager**, a crucial module responsible for distributing rewards from the Omnipools to users. It was a tricky component, and the initial team had struggled with it, which is why I was brought on board.\\n\\nTo be honest, I struggled too. I\u2019ll dive into the technical and mathematical complexity of this module in a later post.\\n\\nAt this stage, Opal was still a side project for everyone, so the relatively slow development pace suited us. But that was about to change.\\n\\n## Fundraising: Private & Public Rounds\\n\\nWhile development was underway, the business team was busy pitching to investors and organizing a private seed round. I wasn\u2019t deeply involved in this phase, but word was that it went well \u2014 we secured commitments without giving up too many tokens, and **early signs from the market were encouraging**.\\n\\nOur [Twitter](https://x.com/opaldefi) and Discord communities started **gaining traction**, adding pressure on the dev team to finalize the protocol.\\n\\nThe funds raised during the private seed were used to ramp up marketing: promo videos, contests, influencer campaigns \u2014 all in preparation for the public round.\\n\\nThe public raise was hosted on [Fjord Foundry](https://app.fjordfoundry.com/token-sales/0x1F272Ab2BDc512cb59e7b49485eFE16d2d7F9ffa), a platform tailored for such events. The response was phenomenal \u2014 we raised $1.58 million. I could hardly believe it myself.\\n\\n![Opal fundraising metrics](/img/opal_fundraise.png)\\n> Opal fundraising metrics. Source: [Fjord Foundry](https://app.fjordfoundry.com/token-sales/0x1F272Ab2BDc512cb59e7b49485eFE16d2d7F9ffa).\\n\\n## Delivering on Expectations \\n\\nWith a successful raise behind us, the pressure was on. Development needed to accelerate. Looking back, this period was rough \u2014 we clearly underestimated the time required to finalize the product.\\n\\nWe went from working 4\u20136 hours a week to over 20 hours per week, all while juggling full-time jobs. We rushed through two audits, each with just five days to fix reported issues. The first audit revealed more flaws than I expected, which was nerve-racking. But if you\'re in a similar situation \u2014 **don\u2019t panic**. It\'s normal, and you\u2019ll likely go through more audits to patch things up.\\n\\nThe machine was running at full speed. We were in the final stretch.\\n\\n## Opal in Production\\n\\nThey say childbirth is one of the most painful human experiences. I won\u2019t compare directly, but launching Opal felt like a drawn-out labor with complications.\\n\\nWhat was supposed to take hours ended up taking a full week due to last-minute issues. One integration test was producing inexplicable results. Exhaustion and stress didn\u2019t help. We delayed the launch to debug the issue, which turned out to be a decimal misconfiguration. Not catastrophic, but enough to **shake our confidence** just hours before going live.\\n\\nStill, we launched. TVL started rising, eventually hitting **$8 million\u2014a historic high**.\\n\\n## The Downside\\n\\nPost-launch, Opal faced criticism. The community expected a more polished product with more features. We had delivered everything planned for V1, so I was surprised. But it may have been more about market conditions and a separate incident with MEV Capital.\\n\\nMEV Capital, one of our biggest backers, had contributed ~80% of our TVL. They decided to withdraw a large amount of the protocol\'s liquidity without warning. Unfortunately, they encountered a bug in the DApp\'s slippage logic on one of the Omnipools. Without simulating the transaction or using a MEV blocker, they suffered an instant and irreversible **six-figure MEV loss**.\\n\\nTensions rose when the Opal team chose not to assume responsibility. That day, our TVL dropped significantly, and the protocol\'s reputation took a major hit.\\n\\nIt\'s hard to accept that the future of the protocol hinged on a small, avoidable mistake\u2014something that could\u2019ve been prevented in countless ways, both on our side and MEV Capital\u2019s. All I can say is: it happened, and it cost us dearly.\\n\\n## Redemption Arc\\n\\nIn the months that followed, we managed to rebuild some momentum. We secured a grant from Balancer, added new features, and brought TVL back to around $600K.\\n\\nWe\'re currently trying to re-energize the project with an airdrop campaign, community incentives, and feature updates.\\n\\nThe project still suffers from its damaged reputation, and the omnipool model has lost some steam \u2014 especially after Conic Finance started to talk about shutting down their protocol.\\n\\nBut we\'re still optimistic. Our product has been battle-tested, and we remain committed to improving and promoting it. With continued effort, Opal might just win people over again."},{"id":"request-throttler","metadata":{"permalink":"/clementjuventin-blog/request-throttler","source":"@site/blog/2024-04-14-request-throttler.md","title":"Tackling Rate Limiting, One of My First Challenges at Cede Labs","description":"Cede Labs\u2019 SDK is designed to support any number of centralized exchange accounts, allowing applications or users to query data from several CEXs seamlessly. However, this flexibility exposed a serious issue: API rate limits.","date":"2024-04-14T00:00:00.000Z","tags":[],"readingTime":4.29,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"request-throttler","title":"Tackling Rate Limiting, One of My First Challenges at Cede Labs","authors":"clementjuventin","tags":[]},"unlisted":false,"prevItem":{"title":"Opal - Yield DApp built on top of Balancer","permalink":"/clementjuventin-blog/opal"},"nextItem":{"title":"I\'m Joining Cede Labs as a Full-Stack Developer!","permalink":"/clementjuventin-blog/welcome-to-cede-labs"}},"content":"Cede Labs\u2019 SDK is designed to support any number of centralized exchange accounts, allowing applications or users to query data from several CEXs seamlessly. However, this flexibility exposed a serious issue: **API rate limits**.\\n\\n\x3c!-- truncate --\x3e\\n\\nOver the past few weeks at **Cede Labs**, I\u2019ve been diving deep into the codebase, getting familiar with the SDK, and preparing myself for the complex technical challenges ahead. One of the first major problems I encountered \u2014 and helped solve \u2014 was **rate limiting across multiple centralized exchanges (CEXs)**.\\n\\n## The Problem: Managing Rate Limits in a Multi-CEX World\\n\\nCede Labs\u2019 SDK is designed to support any number of centralized exchange accounts, allowing applications or users to query data from several CEXs seamlessly. However, this flexibility exposed a serious issue: **API rate limits**.\\n\\nThe tricky part is that **rate limiting strategies vary widely between exchanges**. Some enforce limits by:\\n\\n- IP address\\n- API key\\n- Master/sub-account hierarchies\\n- Or even combinations of all three\\n\\nWe observed this firsthand through our **Chrome extension**, which integrates the SDK to interact with multiple exchanges. With many CEX accounts active simultaneously, we were quickly exhausting rate limits, resulting in **slowness, API errors, and buggy behavior**.\\n\\nClearly, we needed a robust, centralized system to handle API throttling across all exchange instances.\\n\\n## The Solution: Building a Shared Request Throttler\\n\\nOur SDK uses **CCXT**, a widely adopted library that simplifies communication with CEX APIs. CCXT conveniently includes built-in metadata about rate limits \u2014 including cost calculations and endpoint-specific constraints \u2014 which became the foundation for our fix.\\n\\nI built a component called the **Request Throttler** to manage all outgoing requests. This module:\\n\\n- Runs a continuous loop, processing a queue of API requests\\n- Delays or batches calls based on calculated rate limits\\n- Shares throttling logic between all exchange instances tied to the same CEX\\n- Supports custom priorities to fast-track high-importance requests\\n\\n## The Code: A Deep Dive into the Request Throttler\\n\\nBelow is a simplified version of the throttler\'s main loop. For readability, I\u2019ve omitted queue-empty handling (i.e., when `this.getNext()` returns `null`).\\n\\n```typescript\\nasync loop () {\\n    let lastTimestamp = now ();\\n    while (this.running) {\\n        const { resolver, cost, rejecter, timestamp, expireInterval } = this.getNext ();\\n\\n        if (this.tokens >= 0) {\\n            if (timestamp + expireInterval < now ()) {\\n                rejecter (\'Request expired\');\\n            } else {\\n                this.tokens -= cost;\\n                resolver ();\\n            }\\n            await Promise.resolve ();\\n        } else {\\n            await sleep (this.delay);\\n            const current = now ();\\n            const elapsed = current - lastTimestamp;\\n            lastTimestamp = current;\\n            const tokens = this.tokens + this.refillRate * elapsed;\\n            this.tokens = Math.min (tokens, this.capacity);\\n        }\\n    }\\n}\\n```\\n\\n#### Key Concepts\\n- `this.tokens` represents the available request \\"budget\\". It\'s reduced by the `cost` of each request.\\n- If `this.tokens` is below 0, the system will pause (`sleep`) and **refill** tokens based on elapsed time and a configured **refill rate**.\\n- If tokens are available (i.e., `>= 0`), the request is executed **only if it hasn\'t expired** (based on `timestamp + expireInterval`).\\n- The line `await Promise.resolve();` is a **context switch** \u2014 it gives other asynchronous tasks a chance to execute, which helps avoid blocking the event loop in JavaScript.\\n- **Token refill logic** happens after the delay and simulates a \\"leaky bucket\\" or \\"token bucket\\" algorithm, a common strategy in rate-limiting systems.\\n\\nThis loop ensures that all API requests respect the configured rate limits by dynamically adjusting based on real-time usage and availability.\\n\\n## Prioritization and Background Work\\n\\nIn practice, our SDK performs a lot of **background tasks**, such as:\\n\\n- Balance fetching\\n- Historical trade retrieval\\n- Status checks\\n\\nThese operations are necessary but can be expensive in terms of API cost. So, I implemented a **priority system**: important requests (like user-triggered actions) are associated to high priority while background tasks are associated to low priority.\\n\\nThis ensures faster user experiences without breaking rate limits or starving essential tasks.\\n\\n## The Hard Part: Rate Limit Discovery\\n\\nThe most difficult aspect wasn\u2019t writing the throttler logic itself \u2014 it was understanding and standardizing **how each exchange handles rate limits**.\\n\\n- Many rate-limiting rules are hidden, sometimes undocumented and in worse cases not aligned with the official documentation\\n- CCXT\u2019s built-in cost definitions were often outdated\\n- Some CEXs change behavior depending on the account type or endpoint\\n\\nI had to manually test, validate, and correct the API cost values for several exchanges, updating the SDK accordingly.\\n\\nIt also took careful work to **share a throttler** across multiple CEX instances (i.e., multiple ccxt objects) since each has unique constraints, such as caching behavior or read/write permission separation.\\n\\nFor context:\\n> A CEX instance in our system is a `ccxt` object tied to a specific API key and access rights. Each instance needs to respect its rate limits but also coordinate with other instances for the same exchange.\\n\\nThat\u2019s where the Request Throttler comes in \u2014 it sits above the instances and coordinates them like air traffic control.\\n\\n## Looking Ahead\\n\\nThis was a great first challenge at Cede Labs. It pushed me to go beyond just writing code \u2014 I had to understand CEXs inside out, map inconsistencies, and design a system that works under real-world constraints.\\n\\nThere\u2019s still a lot to build and while this system highly reduced our rate limit issues, there is still a lot of improvements to make to reach our goals.\\n\\nI\u2019m proud of how this first technical hurdle turned into a solid foundation for more scalable and reliable SDK behavior.\\n\\nMore to come soon!"},{"id":"welcome-to-cede-labs","metadata":{"permalink":"/clementjuventin-blog/welcome-to-cede-labs","source":"@site/blog/2023-03-02-welcome-to-cede-labs.md","title":"I\'m Joining Cede Labs as a Full-Stack Developer!","description":"I\u2019m thrilled to announce that I\u2019m joining Cede Labs as a Full-Stack Developer!","date":"2023-03-02T00:00:00.000Z","tags":[],"readingTime":1.73,"hasTruncateMarker":true,"authors":[{"name":"Cl\xe9ment Juventin","title":"Internet traveler \ud83e\udd16\u200b","url":"https://linkedin.com/in/cl\xe9ment-juventin-ab81841a3/","page":{"permalink":"/clementjuventin-blog/authors/clementjuventin"},"socials":{"linkedin":"https://www.linkedin.com/in/clementjuventin/","github":"https://github.com/clementjuventin"},"imageURL":"https://github.com/clementjuventin.png","key":"clementjuventin"}],"frontMatter":{"slug":"welcome-to-cede-labs","title":"I\'m Joining Cede Labs as a Full-Stack Developer!","authors":"clementjuventin","tags":[]},"unlisted":false,"prevItem":{"title":"Tackling Rate Limiting, One of My First Challenges at Cede Labs","permalink":"/clementjuventin-blog/request-throttler"}},"content":"I\u2019m thrilled to announce that I\u2019m joining Cede Labs as a Full-Stack Developer!\\n\\n\x3c!-- truncate --\x3e\\n\\nCede Labs is a Paris-based French startup building the missing bridge between **centralized exchanges (CEXs)** and the **decentralized finance (DeFi) ecosystem**. Their mission is to create seamless connectivity between Web2 and Web3 financial systems \u2014 and I\u2019m excited to become a part of that journey.\\n\\n## What Cede Labs does\\n\\nAt its core, Cede Labs offers a powerful Software Development Kit (SDK) that enables developers to interact with over 10 major centralized exchanges (*Binance, Bybit, Coinbase, etc.*) and more than 1,000 supported blockchains. With this SDK, developers can:\\n\\n- Retrieve account and trading history\\n- Fetch balances\\n- Execute trades\\n- Deposit and withdraw assets\\n\\nBut that\'s not all. The team is also building a Chrome extension wallet that integrates directly with their SDK, providing a smooth and user-friendly interface. It enables users to interact with DeFi applications just like they would with a standard Web3 wallet like MetaMask \u2014 but with deeper CEX capabilities under the hood.\\n\\n## Why this Role Excites Me\\n\\nThis opportunity marks my **official entry into the Web3 industry**, something I\'ve been eager to pursue. Even more exciting, it allows me to continue applying my experience in **traditional finance development**, combining the best of both worlds.\\n\\nThe potential use cases for the SDK are vast, and the roadmap ahead is full of innovation \u2014 from enhanced trading features like **futures support** to **onboarding new exchanges**. There\'s a lot to build, and I\'m thrilled to contribute.\\n\\n## What I\'ll Be Working On\\n\\nAs a full-stack developer at Cede Labs, my role spans across both frontend and backend, including:\\n\\n- Developing the Chrome extension UI and adding key features\\n- Contributing to the SDK, enhancing its capabilities and exchange support\\n- Building and maintaining a NestJS API\\n- Managing infrastructure deployments on AWS using Terraform\\n\\n## Looking Ahead\\n\\nThere\u2019s a ton of exciting work ahead, and I can\u2019t wait to share what we\u2019re building at Cede Labs. I\u2019m looking forward to learning from the brilliant team and bringing my own contributions to the table. Stay tuned \u2014 more updates coming soon!"}]}}')}}]);